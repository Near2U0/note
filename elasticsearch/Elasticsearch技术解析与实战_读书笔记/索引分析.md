索引分析是这样一个过程：首先，把一个文本快分析成一个个单独的词（term），为了后面的倒排索引做准备，然后标准化这些词为标准形式，提高他们的可搜索性，这些工作就是分词器所完成的，**一个分词器（analyzers）是一个组合**，用于将下面的三个功能放到一起：

* 字符过滤器：在标记化之前处理字符串，字符过滤器能够去除HTML标记，或者转换“&”为“and”
* 分词器：将文本进行分词，一个简单的分词器可以根据空格或者逗号将单词分开
* 标记过滤器：大小写转换，去掉停词，同义词转换

ES提供了很多内置的字符过滤器，分词器和标记过滤器，我们可以将其组合起来建立自己的分析器以应对不同的需求

```
POST http://localhost:9200/_analyze
{
	"analyzer":"standard",	#指定分词器
	"text":"this is a test"	#被分词的文本
}

#在该分词器下，将会分析成this ， is， a, test 四个词
```

自定义分析器

```
POST http://localhost:9200/_analyze
{
	"tokenizer":"keyword",
	"token_filters":["lowercase"],
	"char_filters":["html_strip"],
	"text":"this is a <b>test</b>"
}
```

组成上面分词器的是：字符过滤器（html_strip),分词器（keyword），分词过滤器（lowercase）


也可以指定索引进行分词，如下：

```
POST http://localhost:9200/secisland/_analyze
{
	"tokenizer":"standard",
	"token_filters":["snowball"],
	"text":"detailed output",
	"explain":true,
	"attributes":"["keyword"]
}

#如果想要获取分词器的分词详情，可以设置explain=true(默认是false),这样将输出分词器的分词详情
```
