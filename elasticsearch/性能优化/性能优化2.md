[TOC]

# Elasticsearch的索引思路



将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数(同时也利用磁盘顺序读特性)，结合各种高级的压缩算法，用及其苛刻的态度使用内存。
所以，对于使用Elasticsearch进行索引时需要注意:

* 不需要索引的字段，一定要明确定义出来，因为默认是自动建索引的，也就是**非查询字段不建索引**

  ```shell
  索引index 
  这个参数可以控制字段应该怎样建索引，怎样查询。它有以下三个可用值： 
  #no: 不把此字段添加到索引中，也就是不建索引，此字段不可查询 
  #not_analyzed:将字段的原始值放入索引中，作为一个独立的term，它是除string字段以外的所有字段的默认值。 
  #analyzed:string字段的默认值，会先进行分析后，再把分析的term结果存入索引中。
  
  那么如果不需要的值，直接在mapping中设置为no就可以了。其它信息参考：store 和 _source field
  
  简要说明：store就是把这个字段单独存储，默认是不存储的，默认存储的是 _source ，只有设置了才会存储，比如，有一个mapping有三个字段，title，subject，content，其中title，subject比较小，而content非常非常大，如果，你的查询结果只需要title，subject而不需要content时，把title，subject单独存储，可以节省很多时间
  
  ```

  

* 同样的道理，对于String类型的字段，不需要analysis的也需要明确定义出来，因为默认也是会analysis的

* 选择有规律的ID很重要，随机性太大的ID(比如java的UUID)不利于查询

关于最后一点，个人认为有多个因素:
其中一个(也许不是最重要的)因素: 上面看到的压缩算法，都是对Posting list里的大量ID进行压缩的，那如果ID是顺序的，或者是有公共前缀等具有一定规律性的ID，压缩比会比较高；
另外一个因素: 可能是最影响查询性能的，应该是最后通过Posting list里的ID到磁盘中查找Document信息的那步，因为Elasticsearch是分Segment存储的，根据ID这个大范围的Term定位到Segment的效率直接影响了最后查询的性能，如果ID是有规律的，可以快速跳过不包含该ID的Segment，从而减少不必要的磁盘读次数

* 优化方式

  ```shell
  #1.include_in_all
  
  
  #2.index 
  	no #不让查询的字段
  	not_analyzed  #原字段，原封不动的添加到索引中
  ```

* 下面是一份完整的优化

  ```shell
  
  =============================================================
  #删除
  curl -XDELETE http://10.130.10.18:9200/_template/netflow_template
  
  #netflow动态模板
  
  curl -XPUT "http://10.119.248.90:9200/_template/netflow_template" -d '{
    "template": "netflow_*",
    "mappings": {
      "default": {
  		 "_source":{
              "includes":["srcip","dstip","protocol","proto7","upbytesize","downbytesize","nodeid"]
          },
  		"_all":{"enabled": false},
  		"properties":{
  			"recordtime": {
  				"format": "strict_date_optional_time||epoch_second",
  				"include_in_all": false,
  				"type": "date"
  			},
      		"srcip": {
      			"type": "ip",
  				"include_in_all": false,
  				"index" : "not_analyzed" 
      		},
      		"protocol": {
      			"type": "string",
  				"include_in_all": false,
      			"index" : "not_analyzed" 
      		},
      		"proto7": {
      			"type": "string",
  				"include_in_all": false,
  				"index" : "not_analyzed" 
      		},
      		"upbytesize": {
      			"type": "long",
  				"include_in_all": false,
  				"index" : "not_analyzed" 
      		},
      		"downbytesize": {
      			"type": "long",
  				"include_in_all": false,
  				"index" : "not_analyzed"
      		},
      		"dstip": {
      			"type": "ip",
  				"include_in_all": false,
  				"index" : "not_analyzed" 
      		},
      		"nodeid": {
      			"type": "long",
  				"include_in_all": false,
  				"index" : "not_analyzed" 
      		},
  			"sprovince":{
  				"type": "string",
  				"include_in_all": false,
      			"index" : "not_analyzed" 
  			},
  			"scountry":{
  				"type": "string",
  				"include_in_all": false,
      			"index" : "not_analyzed" 
  			},
  			"scity": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"dcountry":{
  				"type": "string",
  				"include_in_all": false,
      			"index" : "not_analyzed" 
  			},
  			"dprovince":{
  				"type": "string",
  				"include_in_all": false,
      			"index" : "not_analyzed" 
  			},
  			"dcity": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			
  			
  			"downpkts": {
  				"type": "long",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"uppkts": {
  				"type": "long",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"dstport": {
  				"type": "long",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"srcport": {
  				"type": "long",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			
  			"dlongitude": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"dlatitude": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"slongitude": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"slatitude": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"starttime": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			"endtime": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		},
  			
  			"flow_id": {
  				"type": "string",
  				"include_in_all": false,
  				"index" : "no"
      		}
  			
      	}
      }
    }
  }'
  
  ```

  


# 打开文件太多异常

> 出现“打开文件过多”，解决的方式，通过段合并

转自：https://www.jianshu.com/p/0ceb59025521



在前两次集群扩容的过程中，总是会出现Too many open files in system问题。对于这个问题，困扰了一段时间。由于elasticsearch在非root用户下运行，开始以为只需要调整limit.conf中的句柄数配置就好。但是在我的句柄数已经调到655350之后，还出现这个问题。我通过检查 lsof 命令发现单个节点的句柄数并不在一个数量级。

![img](E:\git-workspace\note\images\es\webp.png)

如上图所示。
现在证明造成打开文件过多的问题的思路并非局限在limit配置。还需要考虑elasticsearch何时会创建大批量的文件？
仔细查看了elasticsearch文档，发现如下描述：

```undefined
由于自动刷新流程每秒会创建一个新的段，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。
Elasticsearch通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。
段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。
```

过程如下图：

![img](E:\git-workspace\note\images\es\webp2.png)



琐碎的段会自动合并为一个大段，在新的大段被创建完成并flash之后，旧的段会被删除。

![img](E:\git-workspace\note\images\es\webp3.png)



因此，这就解释了“Too many open files in system”问题出现的原因：
在系统扩容的过程中，会有大量的数据被平衡到新的节点，这样会消耗大量的IO，同时，elk集群中的新数据，由于没有对数据节点做冷热区分，会源源不断的写入到新节点，这就造成了新节点中的段会非常多，旧的段无法合并，新的数据又在源源不断的写入，这就造成了文件数会越来越多，因此出现了上述问题。

要想彻底解决上述问题，只有一个办法，那就是定期对段进行合并。elk官方建议段的数量是一个分片只有一个。
可以通过如下api进行设置：

```undefined
POST /{indexname}/_forcemerge?max_num_segments=1
```

还支持通配符：

```undefined
POST /applog-prod-2016.12.*/_forcemerge?max_num_segments=1
```

可以通过如下API查看索引的段信息：

```undefined
GET applog-prod-2016.11.30*/_segments
```

如下是段合并的效果：

![img](E:\git-workspace\note\images\es\webp5.png)

段监控图

总结：
1.需要对elasticsearch制定冷/热策略，将节点分为存放历史数据的冷节点，和存放实时数据的热节点。
2.需要定时对冷数据进行段合并。



* 通过脚本每天定时进行段的合并

```shell
#定时对ES的指定index做段合并 脚本

#需要做段合并的index： netflow ,http, dns, syslog


可以通过如下api进行设置：

#!/bin/bash
#es config
es_host=10.130.10.18

#index name
yesterday_date=`date  "+%Y%m%d" -d "-1day"`
index_netflow=netflow_$yesterday_date
index_http=http_$yesterday_date
index_dns=dns_$yesterday_date
index_syslog=syslog_$yesterday_date

index_array=($index_netflow $index_http $index_dns $index_syslog)

for index_name in "${index_array[@]}";do
    echo $index_name
    #is exist
    cnt=`curl -XHEAD -i http://10.130.10.18:9200/$index_name/$index_name|grep 200|wc -l`
    if [ $cnt -gt 0 ];then
    	echo "http://$es_host:9200/$index_name/_forcemerge?max_num_segments=1"
    	#curl -XPOST http://$es_host:9200/$index_name/_forcemerge?max_num_segments=1
    fi
    #15min
	sleep 900
done


```


