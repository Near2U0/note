[TOC]



记一次ES内存问题引起的ES的OOM

# 背景及问题

我们的流量数据是按天存储在ES中的，数据量非常的大，截图如下

![1565059605056](E:\git-workspace\note\images\es\1565059605056.png)

在ES的后台有如下的日志，会发生jvm内存回收的

在ES的后台日志中看到好多jvm的内存回收的日志

![1565059605056](E:\git-workspace\note\images\es\1565060024810.png)

解析一下上面一条日志的含义：

```shell
[2017-03-06T00:09:15,151][INFO ][o.e.m.j.JvmGcMonitorService] [GnqLs1r] [gc][old][98590][55] duration [7.9s], collections [1]/[8.1s], total [7.9s]/[34.7s], memory [7.7gb]->[7.1gb]/[7.9gb], all_pools {[young] [492.6mb]->[1.6mb]/[532.5mb]}{[survivor] [66.5mb]->[0b]/[66.5mb]}{[old] [7.1gb]->[7.1gb]/[7.3gb]}
```

这是 Elasticsearch 在进行内存回收时记录的日志。我们解析下日志可以得到以下信息：

| 日志内容                               | 解析                                           |
| :------------------------------------- | :--------------------------------------------- |
| `duration [7.9s]`                      | 垃圾回收器消耗的总时长为7.9秒                  |
| `memory [7.7gb]->[7.1gb]/[7.9gb]`      | 内存使用从7.7G回收到了7.1G，总共可用内存为7.9G |
| `[young] [492.6mb]->[1.6mb]/[532.5mb]` | 新生代内存回收情况                             |
| `[survivor] [66.5mb]->[0b]/[66.5mb]`   | 中生代内存回收情况                             |
| `[old] [7.1gb]->[7.1gb]/[7.3gb]`       | 老生代内存回收情况                             |

我们注意到，内存回收已经到达瓶颈，可回收的内存只占可用内存的1/8，这种情况随着查询的进行还会更加严重。这就是产生 OOM 异常的原因。

另外，老生代是内存占用的主力军，说明查询的跨度大，内存更迭频率高，大量的内存被移入老生代区域。

# 解决方案

1. 提高系统的内存

到此，我们找到了 OOM 产生的原因。为了找寻解决方案，我模拟了统计工具的执行环境，查看当时 Elasticsearch 的内存使用分布。发现，几乎所有占用的内存都是 Segment Memory，该内存是为了提高查询速度，Lucene 将索引部分加载到内存中。这意味着，查询的数据量决定了内存的使用。

再来看下，我们执行统计需要的数据量。目前，我们每天统计的日志量在2~3G，同时查询的天数最多有30天。面对这样量级的查询，需要装载的索引大小超过我们给它分配的8G，也是极有可能的。

分析到此，当下解决问题最好的方案：加内存。将内存扩容到32G，并且分配16G给 Elasticsearch，统计时出现 OOM 异常就暂时消失了。



2. 只是保存部署数据

将以前的数据做了静态备份，将对应index的数据copy到一个目录下，然后删除ES中的索引

![1565059605056](E:\git-workspace\note\images\es\1565060417957.png)

之所以可以删除这部分数据，是因为，我们流量的数据只是用来统计，统计之后就没有作用了，所以只需要保存7天或者1天的数据，下面是一个脚本，每天定时删除数据

```shell
#流量数据只存储一天的数据
*/1 * * * * /usr/bin/curl  -XDELETE soc60:9200/netflow_20190804 >/dev/null  2>&1
```

