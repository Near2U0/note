---
title: 网站访问次数
categories: spark  
tags: [spark]
---



# 1.数据格式
```
 
/*数据格式:其中java.itcast.cn表示Java学院,net.itcast.cn表示net学院,php.itcast.cn表示php学院...
20160321102002 http://java.itcast.cn/java/course/javaee.shtml
20160321102002 http://net.itcast.cn/net/teacher.shtml
20160321102002 http://php.itcast.cn/php/teacher.shtml
*/

```

# 2需求
&emsp;取出各个学院点击前三的网页

<!--more-->

# 3.代码实现1

```
package cn.itcast.spark.day2

import java.net.URL

import org.apache.spark.{SparkConf, SparkContext}



object UrlCount {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("UrlCount").setMaster("local[2]")
    val sc = new SparkContext(conf)

    //rdd1将数据切分，元组中放的是（URL， 1）
    val rdd1 = sc.textFile("D:\\itcast-大数据\\day29\\itcast.log").map(line => {
      val f = line.split("\t")
      //(http://php.itcast.cn/php/teacher.shtml, 1)
      (f(1), 1)
    })

    //(http://php.itcast.cn/php/teacher.shtml, 10)
    val rdd2 = rdd1.reduceByKey(_+_)

    val rdd3 = rdd2.map(t => {//(http://php.itcast.cn/php/teacher.shtml, 10)
      val url = t._1
      val host = new URL(url).getHost
      //(php.itcast.cn, http://php.itcast.cn/php/teacher.shtml, 10)
      (host, url, t._2)
    })


    //(php.itcast.cn, List((php.itcast.cn, http://php.itcast.cn/php/teacher.shtml, 10), (php.itcast.cn, http://php.itcast.cn/php/course.shtml, 30)))
    val rdd4 = rdd3.groupBy(_._1)

    val rdd5 = rdd4.mapValues(it => {// (php.itcast.cn, http://php.itcast.cn/php/course.shtml, 30)
      it.toList.sortBy(_._3).reverse.take(3)
    })

    println(rdd2.collect().toBuffer)
    sc.stop()
  }
}

```


# 4.代码实现2
```
package cn.itcast.spark.day2

import java.net.URL

import org.apache.spark.{SparkConf, SparkContext}

/**
将每个学院单独拿出来形成一个单独的rdd,这样当内存不够的时候,可以将数据写入到磁盘,不至于将内存撑爆
  */
object AdvUrlCount {

  def main(args: Array[String]) {

    //从数据库中加载规则
    val arr = Array("java.itcast.cn", "php.itcast.cn", "net.itcast.cn")

    val conf = new SparkConf().setAppName("AdvUrlCount").setMaster("local[2]")
    val sc = new SparkContext(conf)
    //rdd1将数据切分，元组中放的是（URL， 1）
    val rdd1 = sc.textFile("c://itcast.log").map(line => {
      val f = line.split("\t")
      (f(1), 1)
    })
    val rdd2 = rdd1.reduceByKey(_ + _)

    val rdd3 = rdd2.map(t => {
      val url = t._1
      val host = new URL(url).getHost
      (host, url, t._2)
    })

    //println(rdd3.collect().toBuffer)

//    val rddjava = rdd3.filter(_._1 == "java.itcast.cn")
//    val sortdjava = rddjava.sortBy(_._3, false).take(3)
//    val rddphp = rdd3.filter(_._1 == "php.itcast.cn")

    for (ins <- arr) {
      val rdd = rdd3.filter(_._1 == ins)//形成单独的rdd
      val result= rdd.sortBy(_._3, false).take(3)
      //通过JDBC向数据库中存储数据
      //id，学院，URL，次数， 访问日期
      println(result.toBuffer)
    }

    //println(sortdjava.toBuffer)
    sc.stop()

  }
}
```