---
title: 第4章 存储系统
categories: spark   
toc: true  
tag: [spark]
---

spark为了避免Hadoop读写磁盘的I/O操作成为性能瓶颈,优先将配置信息,计算结果等数据存入内存,这极大的提升了系统的执行效率,正是因为这一关键决策,才让spark能在大数据应用中表现出优秀的计算能力

<!--more-->


# 1.存储系统概述

## 1.1.块管理器BlockManager的实现
块管理器BlockManager是spark存储体系中的核心组件,因此本章主要围绕BlockManager展开,Driver Application和Executor都会穿件BlockManager,BlockManager的实现见代码:
```

  val diskBlockManager = new DiskBlockManager(this, conf)

  private val blockInfo = new TimeStampedHashMap[BlockId, BlockInfo]

  private val futureExecutionContext = ExecutionContext.fromExecutorService(
    ThreadUtils.newDaemonCachedThreadPool("block-manager-future", 128))

  // Actual storage of where blocks are kept
  private var externalBlockStoreInitialized = false
  private[spark] val memoryStore = new MemoryStore(this, memoryManager)
  private[spark] val diskStore = new DiskStore(this, diskBlockManager)
  private[spark] lazy val externalBlockStore: ExternalBlockStore = {
    externalBlockStoreInitialized = true
    new ExternalBlockStore(this, executorId)
  }
  memoryManager.setMemoryStore(memoryStore)

  // Note: depending on the memory manager, `maxStorageMemory` may actually vary over time.
  // However, since we use this only for reporting and logging, what we actually want here is
  // the absolute maximum value that `maxStorageMemory` can ever possibly reach. We may need
  // to revisit whether reporting this value as the "max" is intuitive to the user.
  private val maxMemory = memoryManager.maxStorageMemory

  private[spark]
  val externalShuffleServiceEnabled = conf.getBoolean("spark.shuffle.service.enabled", false)

  // Port used by the external shuffle service. In Yarn mode, this may be already be
  // set through the Hadoop configuration as the server is launched in the Yarn NM.
  private val externalShuffleServicePort = {
    val tmpPort = Utils.getSparkOrYarnConfig(conf, "spark.shuffle.service.port", "7337").toInt
    if (tmpPort == 0) {
      // for testing, we set "spark.shuffle.service.port" to 0 in the yarn config, so yarn finds
      // an open port.  But we still need to tell our spark apps the right port to use.  So
      // only if the yarn config has the port set to 0, we prefer the value in the spark config
      conf.get("spark.shuffle.service.port").toInt
    } else {
      tmpPort
    }
  }

  var blockManagerId: BlockManagerId = _

  // Address of the server that serves this executor's shuffle files. This is either an external
  // service, or just our own Executor's BlockManager.
  private[spark] var shuffleServerId: BlockManagerId = _

  // Client to read other executors' shuffle files. This is either an external service, or just the
  // standard BlockTransferService to directly connect to other Executors.
  private[spark] val shuffleClient = if (externalShuffleServiceEnabled) {
    val transConf = SparkTransportConf.fromSparkConf(conf, "shuffle", numUsableCores)
    new ExternalShuffleClient(transConf, securityManager, securityManager.isAuthenticationEnabled(),
      securityManager.isSaslEncryptionEnabled())
  } else {
    blockTransferService
  }

  // Whether to compress broadcast variables that are stored
  private val compressBroadcast = conf.getBoolean("spark.broadcast.compress", true)
  // Whether to compress shuffle output that are stored
  private val compressShuffle = conf.getBoolean("spark.shuffle.compress", true)
  // Whether to compress RDD partitions that are stored serialized
  private val compressRdds = conf.getBoolean("spark.rdd.compress", false)
  // Whether to compress shuffle output temporarily spilled to disk
  private val compressShuffleSpill = conf.getBoolean("spark.shuffle.spill.compress", true)

  private val slaveEndpoint = rpcEnv.setupEndpoint(
    "BlockManagerEndpoint" + BlockManager.ID_GENERATOR.next,
    new BlockManagerSlaveEndpoint(rpcEnv, this, mapOutputTracker))

  // Pending re-registration action being executed asynchronously or null if none is pending.
  // Accesses should synchronize on asyncReregisterLock.
  private var asyncReregisterTask: Future[Unit] = null
  private val asyncReregisterLock = new Object

  private val metadataCleaner = new MetadataCleaner(
    MetadataCleanerType.BLOCK_MANAGER, this.dropOldNonBroadcastBlocks, conf)
  private val broadcastCleaner = new MetadataCleaner(
    MetadataCleanerType.BROADCAST_VARS, this.dropOldBroadcastBlocks, conf)

  // Field related to peer block managers that are necessary for block replication
  @volatile private var cachedPeers: Seq[BlockManagerId] = _
  private val peerFetchLock = new Object
  private var lastPeerFetchTime = 0L

  /* The compression codec to use. Note that the "lazy" val is necessary because we want to delay
   * the initialization of the compression codec until it is first used. The reason is that a Spark
   * program could be using a user-defined codec in a third party jar, which is loaded in
   * Executor.updateDependencies. When the BlockManager is initialized, user level jars hasn't been
   * loaded yet. */
  private lazy val compressionCodec: CompressionCodec = CompressionCodec.createCodec(conf)

//...

```

上面的代码中声明的BlockInfo:TimeStampedHashMap[BlockId, BlockInfo],用于BlockManager缓存BlockId以及对应的BlockInfo,从上面的代码可以看到,BlockManager主要由以下部分组成:
1.shuffle客户端ShuffleClient
2.BlockManagerMaster(对存在于所有Executor上的BlockManager统一管理)
3.磁盘块管理器DiskBlockManager
4.内存存储MemoryStore
5.磁盘存储DiskStore
6.Tachyon存储TachyonStore
7.非广播block清理器metadataCleaner和广播Block清理器BroadcastCleaner
8.压缩算法实现CompressionCodec


BlockManager要生效,必须要初始化,他的初始化方法如下:
```


  /**
   * Initializes the BlockManager with the given appId. This is not performed in the constructor as
   * the appId may not be known at BlockManager instantiation time (in particular for the driver,
   * where it is only learned after registration with the TaskScheduler).
   *
   * This method initializes the BlockTransferService and ShuffleClient, registers with the
   * BlockManagerMaster, starts the BlockManagerWorker endpoint, and registers with a local shuffle
   * service if configured.
   */
  def initialize(appId: String): Unit = {
    blockTransferService.init(this)
    shuffleClient.init(appId)

    blockManagerId = BlockManagerId(
      executorId, blockTransferService.hostName, blockTransferService.port)

    shuffleServerId = if (externalShuffleServiceEnabled) {
      logInfo(s"external shuffle service port = $externalShuffleServicePort")
      BlockManagerId(executorId, blockTransferService.hostName, externalShuffleServicePort)
    } else {
      blockManagerId
    }

    master.registerBlockManager(blockManagerId, maxMemory, slaveEndpoint)

    // Register Executors' configuration with the local shuffle service, if one should exist.
    if (externalShuffleServiceEnabled && !blockManagerId.isDriver) {
      registerWithExternalShuffleServer()
    }
  }

```


BlockManager的初始化步骤如下:
1.BlockTransferService的初始化和ShuffleClient的初始化,参见本章第2节,ShuffleClient默认是BlockTransferService,当有外部的ShuffleService时,调用外部ShuffleService的初始化方法
2.BlockManagerId和ShuffleServerId的创建,当有外部的ShuffleService时,创建新的BlockManagerId,否则ShuffleServiceId默认使用当前BlockManager的BlockManagerId
3.向BlockManagerMaster注册BlockManagerId,具体实现见本章3.3节(当有外部的ShuffleService时,还需要向BlockManagerMaster注册ShuffleServerId)




## 1.2.spark存储体系架构
在详细介绍存储体系之前,我们先用图说明Spark存储体系的架构

![](/assert/img/bigdata/深入理解spark核心思想与源码分析/4/spark_store.png)

* 标号1表示Executor的BlockManager与Driver的BlockManager进行消息通信,例如,注册BlockManager,更新Block信息,获取Block所在的BlockManager,删除Executor等
* 标号2表示BlockManager的读操作(例如get,doGetLocal,BlockManager内部进行的MemoryStore,DiskStore,Tachyon的getgetBytes,getValues等操作)和写操作(例如doPut,putSingle,putBytes以及BlockManager内部进行的MemoryStore,DiskStore,Tachyon的getgetBytes,putArray,putIterator等操作)
* 标号3表示当memoryStore的内存不足时,写入DiskStore,而DiskStore实际依赖于DiskBlockManager
* 标号4表示通过访问远端节点的Executor的BlockManager中的TransportServer提供的RPC服务下载或者上传Block
* 标号4表示远端节点的Executor的BlockManager访问本地Executor的BlockManager中的TransportServer提供的RPC服务下载或者上传Block
* 标号6表示当存储系统选择Tachyon作为存储时,对于BlockManager的读写操作实际调用了TachyonStore的putBytes,putArray,putIterator,getBytes,getValues等

spark目前支持HDFS,Amazon S3两种主流的分布式存储系统,还使用也诞生于UCBerkeley的AMP实验室的Tachyon这种高效的分布式文件系统作为缓存

spark定义了抽象类BlockStore,用于指定所有存储类型的规范,目前BlockStore的具体实现包括MemoryStore,DiskStore和TachyonStore,BlockStore的继承系统如下

![](/assert/img/bigdata/深入理解spark核心思想与源码分析/4/spark_store_class.png)


# 2.shuffle服务与客户端

有人可能为问?为何要将Netty实现的网络服务组件也放到存储体系里面,这是由于spark是分布式部署的,每个Task最终都运行在不同的机器节点上,map任务的输出结构直接存储到map任务所在机器的存储体系中,reduce任务极有可能不再同一机器上运行,所以需要远程下载map任务的中间输出,因此将ShuffleClient放到存储体系是最合适的

ShuffleClient并不像他的名字一样,是shuffle的客户端,他不光是将shuffle文件上传到其他Executor或者下载到本地的客户端,也提供了可以被其他Executor访问的shuffle服务,从第1节的代码中可以知道:当有外部的ShuffleClient时,新建ExternalShuffleClient,否则默认为BlockTransferService,BlockTransferService只有在其init方法被调用,即被初始化才提供服务,以默认的NettyBlockTransferService的init方法为例,如下代码:
```
  override def init(blockDataManager: BlockDataManager): Unit = {
    val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager)
    var serverBootstrap: Option[TransportServerBootstrap] = None
    var clientBootstrap: Option[TransportClientBootstrap] = None
    if (authEnabled) {
      serverBootstrap = Some(new SaslServerBootstrap(transportConf, securityManager))
      clientBootstrap = Some(new SaslClientBootstrap(transportConf, conf.getAppId, securityManager,
        securityManager.isSaslEncryptionEnabled()))
    }
    transportContext = new TransportContext(transportConf, rpcHandler)
    clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava)
    server = createServer(serverBootstrap.toList)
    appId = conf.getAppId
    logInfo("Server created on " + server.getPort)
  }

```

NettyBlockTransferService的初始化步骤如下:
1.创建RpcServer
2.构造TransportContext
3.创建RPC客户端工厂TransportClientFactory
4.创建Netty服务器TransportServer

接下来我们逐步讲解Block的RPC服务,构造TransportContext,创建RPC客户端工厂TransportClientFactory,创建Netty服务器TransportServer的实现,此外还会介绍reduce任务是如何拉取map任务中间结果的(即shuffle过程的数传输)


## 2.1.Block的RPC服务
当map任务和reduce任务处于不同节点时,reduce任务需要从远端节点下载map任务的中间输出结果,因此NettyBlockRpcServer提供打开,即下载Block文件的功能,一些情况下,为了容错,需要将Block的数据备份到其他节点上,所以NettyBlockRPCServer还提供了上传Block文件的RPC服务,NettyBlockRPCServer的实现代码如下:
```

class NettyBlockRpcServer(
    appId: String,
    serializer: Serializer,
    blockManager: BlockDataManager)
  extends RpcHandler with Logging {

  private val streamManager = new OneForOneStreamManager()

  override def receive(
      client: TransportClient,
      rpcMessage: ByteBuffer,
      responseContext: RpcResponseCallback): Unit = {
    val message = BlockTransferMessage.Decoder.fromByteBuffer(rpcMessage)
    logTrace(s"Received request: $message")

    message match {
      case openBlocks: OpenBlocks =>
        val blocks: Seq[ManagedBuffer] =
          openBlocks.blockIds.map(BlockId.apply).map(blockManager.getBlockData)
        val streamId = streamManager.registerStream(appId, blocks.iterator.asJava)
        logTrace(s"Registered streamId $streamId with ${blocks.size} buffers")
        responseContext.onSuccess(new StreamHandle(streamId, blocks.size).toByteBuffer)

      case uploadBlock: UploadBlock =>
        // StorageLevel is serialized as bytes using our JavaSerializer.
        val level: StorageLevel =
          serializer.newInstance().deserialize(ByteBuffer.wrap(uploadBlock.metadata))
        val data = new NioManagedBuffer(ByteBuffer.wrap(uploadBlock.blockData))
        blockManager.putBlockData(BlockId(uploadBlock.blockId), data, level)
        responseContext.onSuccess(ByteBuffer.allocate(0))
    }
  }

  override def getStreamManager(): StreamManager = streamManager
}

```


## 2.2.构造传输上下文TransportContext
TransportContext用于维护传输上下文,他的构造器如下:
```
 public TransportContext(
     TransportConf conf,
     RpcHandler rpcHandler,
     boolean closeIdleConnections) {
   this.conf = conf;
   this.rpcHandler = rpcHandler;
   this.encoder = new MessageEncoder();
   this.decoder = new MessageDecoder();
   this.closeIdleConnections = closeIdleConnections;
 }


```

TransportContext既可以创建Netty服务,也可以创建Netty访问客户端,TransportContext的组成如下:
1.TransportConf:主要控制Netty框架提供的shuffle的I/O交互的客户端和服务端线程数量
2.RpcHandler:负责shuffle的I/O服务端在接收到客户端的RPC请求后,提供打开Block或者上传Block的RPC处理,此处即为NettyBlockRPCServer
3.decoder:在shuffle的I/O服务器端对客户端传来的ByteBuff进行行解析,防止丢包和解析错误
4.encoder:在shuffle的I/O客户端对消息内容进行编码的时候,防止服务端丢包和解析错误

问题:为什么需要MessageEncoder和MessageDecode?
因为在基于流的传输里(比如TCP/IP),接收到的数据首先会被存储到一个socket接收缓冲里,不幸的是,基于流的传输并不是一个数据包队列的,而是一个字节队列的,即使发送了2个独立的数据包,操作系统也不会作为2个消息处理,而仅仅认为是一连串的字节,因此不能保证远程写入的数据会被准确的读取,举个例子,假设操作系统的TCP/IP协议栈已经接收到3个数据包,ABC,DEF,GHI,由于基于流传输的协议的这种统一的性质,在应用程序读取数据时很可能性被分成下面的片段:AB,CDEFG,H,I,因此接受方不管是客户端还是服务端,都应该把接收到的数据整理成一个或者多个更有意义并且让程序的逻辑更好理解的数据,这才有了编码和解码


## 2.3.RPC客户端工厂TransportClientFactory

TransportClientFactory是创建Netty客户端TransportClient的工厂类,TransportClient用于向Netty服务器端发送RPC请求,TransportContext的createClientFactory方法用于创建TransportClientFactory
```
  public TransportClientFactory createClientFactory(List<TransportClientBootstrap> bootstraps) {
    return new TransportClientFactory(this, bootstraps);
  }

```

TransportClientFactory的实现如下:
```
  public TransportClientFactory(
      TransportContext context,
      List<TransportClientBootstrap> clientBootstraps) {
    this.context = Preconditions.checkNotNull(context);
    this.conf = context.getConf();
    this.clientBootstraps = Lists.newArrayList(Preconditions.checkNotNull(clientBootstraps));
    this.connectionPool = new ConcurrentHashMap<SocketAddress, ClientPool>();
    this.numConnectionsPerPeer = conf.numConnectionsPerPeer();
    this.rand = new Random();

    IOMode ioMode = IOMode.valueOf(conf.ioMode());
    this.socketChannelClass = NettyUtils.getClientChannelClass(ioMode);
    // TODO: Make thread pool name configurable.
    this.workerGroup = NettyUtils.createEventLoop(ioMode, conf.clientThreads(), "shuffle-client");
    this.pooledAllocator = NettyUtils.createPooledByteBufAllocator(
      conf.preferDirectBufs(), false /* allowCache */, conf.clientThreads());
  }

```
TransportClientFactory由以下部分组成:
1.clientBootstraps:用于缓存客户端列表
2.connectionPool:用于缓存客户端连接
3.numConnectionsPerPeer:节点之前取数据的连接数,可以使用属性spark.shuffle.io.numConnectionsPerPeer来配置,默认为1
4.socketChannelClass:客户端channel被创建时使用的类,可以使用属性spark.shuffle.io.mode来配置,默认为NioSocketChannel
5.workerGroup:根据Netty的规范,客户端只有work组,所以此处创建workerGroup,实际上是NioEventLoopGroup
6.pooledAllocator:汇集ByteBuff但对本地线程缓存禁用的分配器



## 2.4.Netty服务器TransportServer
TransportServer提供了Netty实现的服务器端,用于提供RPC服务(比如上传,下载等),创建TransportServer的代码如下在NettyBlockTransferService的init方法中:
```
server = createServer(serverBootstrap.toList)

//具体方法实现
  /** Creates and binds the TransportServer, possibly trying multiple ports. */
  private def createServer(bootstraps: List[TransportServerBootstrap]): TransportServer = {
    def startService(port: Int): (TransportServer, Int) = {
      val server = transportContext.createServer(port, bootstraps.asJava)
      (server, server.getPort)
    }

    val portToTry = conf.getInt("spark.blockManager.port", 0)
    Utils.startServiceOnPort(portToTry, startService, conf, getClass.getName)._1
  }

```

TransportServer的构造器实现如下:
```
  public TransportServer(
      TransportContext context,
      String hostToBind,
      int portToBind,
      RpcHandler appRpcHandler,
      List<TransportServerBootstrap> bootstraps) {
    this.context = context;
    this.conf = context.getConf();
    this.appRpcHandler = appRpcHandler;
    this.bootstraps = Lists.newArrayList(Preconditions.checkNotNull(bootstraps));

    try {
      init(hostToBind, portToBind);
    } catch (RuntimeException e) {
      JavaUtils.closeQuietly(this);
      throw e;
    }
  }


```

上面的init方法用于对TransportServer初始化,通过使用Netty框架的EventLoopGroup,ServerBootstrap等API创建shuffle的I/O交互的服务器端,init的主要代码如下:
```

  private void init(String hostToBind, int portToBind) {

    IOMode ioMode = IOMode.valueOf(conf.ioMode());
    EventLoopGroup bossGroup =
      NettyUtils.createEventLoop(ioMode, conf.serverThreads(), "shuffle-server");
    EventLoopGroup workerGroup = bossGroup;

    PooledByteBufAllocator allocator = NettyUtils.createPooledByteBufAllocator(
      conf.preferDirectBufs(), true /* allowCache */, conf.serverThreads());

    bootstrap = new ServerBootstrap()
      .group(bossGroup, workerGroup)
      .channel(NettyUtils.getServerChannelClass(ioMode))
      .option(ChannelOption.ALLOCATOR, allocator)
      .childOption(ChannelOption.ALLOCATOR, allocator);

    if (conf.backLog() > 0) {
      bootstrap.option(ChannelOption.SO_BACKLOG, conf.backLog());
    }

    if (conf.receiveBuf() > 0) {
      bootstrap.childOption(ChannelOption.SO_RCVBUF, conf.receiveBuf());
    }

    if (conf.sendBuf() > 0) {
      bootstrap.childOption(ChannelOption.SO_SNDBUF, conf.sendBuf());
    }

    bootstrap.childHandler(new ChannelInitializer<SocketChannel>() {
      @Override
      protected void initChannel(SocketChannel ch) throws Exception {
        RpcHandler rpcHandler = appRpcHandler;
        for (TransportServerBootstrap bootstrap : bootstraps) {
          rpcHandler = bootstrap.doBootstrap(ch, rpcHandler);
        }
        context.initializePipeline(ch, rpcHandler);
      }
    });

    InetSocketAddress address = hostToBind == null ?
        new InetSocketAddress(portToBind): new InetSocketAddress(hostToBind, portToBind);
    channelFuture = bootstrap.bind(address);
    channelFuture.syncUninterruptibly();

    port = ((InetSocketAddress) channelFuture.channel().localAddress()).getPort();
    logger.debug("Shuffle server started on port :" + port);
  }

```

ServerBootstrap的childHandler方法调用了TransportContext的initializePipeline,initializePipeline中创建了TransportChannelHandler,并将它绑定到SocketChannel的pipeline的handler中

```
  public TransportChannelHandler initializePipeline(
      SocketChannel channel,
      RpcHandler channelRpcHandler) {
    try {
      TransportChannelHandler channelHandler = createChannelHandler(channel, channelRpcHandler);
      channel.pipeline()
        .addLast("encoder", encoder)
        .addLast(TransportFrameDecoder.HANDLER_NAME, NettyUtils.createFrameDecoder())
        .addLast("decoder", decoder)
        .addLast("idleStateHandler", new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000))
        // NOTE: Chunks are currently guaranteed to be returned in the order of request, but this
        // would require more logic to guarantee if this were not part of the same event loop.
        .addLast("handler", channelHandler);
      return channelHandler;
    } catch (RuntimeException e) {
      logger.error("Error while initializing Netty pipeline", e);
      throw e;
    }
  }

```

## 2.5.获取远程shuffle文件
NettyBlockTransferService的fetchBlock方法用于获取远程shuffle文件,实际是利用NettyBlockTransferService中创建的Netty服务


```

  override def fetchBlocks(
      host: String,
      port: Int,
      execId: String,
      blockIds: Array[String],
      listener: BlockFetchingListener): Unit = {
    logTrace(s"Fetch blocks from $host:$port (executor id $execId)")
    try {
      val blockFetchStarter = new RetryingBlockFetcher.BlockFetchStarter {
        override def createAndStart(blockIds: Array[String], listener: BlockFetchingListener) {
          val client = clientFactory.createClient(host, port)
          new OneForOneBlockFetcher(client, appId, execId, blockIds.toArray, listener).start()
        }
      }

      val maxRetries = transportConf.maxIORetries()
      if (maxRetries > 0) {
        // Note this Fetcher will correctly handle maxRetries == 0; we avoid it just in case there's
        // a bug in this code. We should remove the if statement once we're sure of the stability.
        new RetryingBlockFetcher(transportConf, blockFetchStarter, blockIds, listener).start()
      } else {
        blockFetchStarter.createAndStart(blockIds, listener)
      }
    } catch {
      case e: Exception =>
        logError("Exception while beginning fetchBlocks", e)
        blockIds.foreach(listener.onBlockFetchFailure(_, e))
    }
  }

```

## 2.6.上传shuffle文件
NettyBlockTransferService的uploadBlock方法用于上传shuffle文件到远程Executor,实际也是利用NettyBlockTransferService中创建的Netty服务,代码如下:

```

  override def uploadBlock(
      hostname: String,
      port: Int,
      execId: String,
      blockId: BlockId,
      blockData: ManagedBuffer,
      level: StorageLevel): Future[Unit] = {
    val result = Promise[Unit]()
    val client = clientFactory.createClient(hostname, port)

    // StorageLevel is serialized as bytes using our JavaSerializer. Everything else is encoded
    // using our binary protocol.
    val levelBytes = serializer.newInstance().serialize(level).array()

    // Convert or copy nio buffer into array in order to serialize it.
    val nioBuffer = blockData.nioByteBuffer()
    val array = if (nioBuffer.hasArray) {
      nioBuffer.array()
    } else {
      val data = new Array[Byte](nioBuffer.remaining())
      nioBuffer.get(data)
      data
    }

    client.sendRpc(new UploadBlock(appId, execId, blockId.toString, levelBytes, array).toByteBuffer,
      new RpcResponseCallback {
        override def onSuccess(response: ByteBuffer): Unit = {
          logTrace(s"Successfully uploaded block $blockId")
          result.success((): Unit)
        }
        override def onFailure(e: Throwable): Unit = {
          logError(s"Error while uploading block $blockId", e)
          result.failure(e)
        }
      })

    result.future
  }

```

NettyBlockTransferService上传Block的步骤如下:
1.创建Netty服务的客户端,客户端连接的hostname和port正是我们随机选择的BlockManager的hostname和port
2.将Block的存储级别StorageLevel序列化
3.将Block的ByteBuff转换为数组,便于序列化
4.将appId,execId,blockId,序列化的StorageLevel,转换为数组的Block封装为uploadBlock,并将UploadBlock序列化为字节数组
5.最终调用Netty客户端的sendRPC方法将字节数组上传,回调函数RPCResponseCallback根据RPC的结果更改上传状态


# 3.BlockManagerMaster对BlockManager的管理

Driver山的BlockManagerMaster对存在于Executor山的BlockManager统一管理,比如Executor需要向Driver发送注册BlockManager,更新Executor山的Block的最新信息,询问所需要Block目前所在位置以及当Executor运行结束需要将此Executor移除等,但是Driver与Executor却位于不同机器中,该怎么实现呢?Driver上的BlockManagerMaster会持有driverEndpoint,所有Executor也会中的BlockManager中也会持有slaveEndpoint,所以Executor与Driver才可以进行关于BlockManager的交互


## 3.1.driverEndpoint
driverEndpoint只是存在于Driver上,Executor从rpcEnv获取slaveEndpoint,然后给driverEndpoint发送消息,实现和Driver的交互,在BlockManagerMaster.driverEndpoint的各种操作代码如下:

```
//get操作,通过driverEndpoint去获取BlockManager的信息
  def getMemoryStatus: Map[BlockManagerId, (Long, Long)] = {
    driverEndpoint.askWithRetry[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus)
  }

  def getStorageStatus: Array[StorageStatus] = {
    driverEndpoint.askWithRetry[Array[StorageStatus]](GetStorageStatus)
  }


//remove操作,也是通过操作driverEndpoint
  def removeRdd(rddId: Int, blocking: Boolean) {
    val future = driverEndpoint.askWithRetry[Future[Seq[Int]]](RemoveRdd(rddId))

	//...
  }

```


##　3.2.询问Driver并获取恢复方法

在Executor的BlockManageMaster中,所有与Driver上BlockManagerMaster的交互方法最终都调用了askWithRetry,可见他是一个最基础的方法,因为代码如下:
```

  def askWithRetry[T: ClassTag](message: Any, timeout: RpcTimeout): T = {
    // TODO: Consider removing multiple attempts
    var attempts = 0
    var lastException: Exception = null
    while (attempts < maxRetries) {
      attempts += 1
      try {
        val future = ask[T](message, timeout)
        val result = timeout.awaitResult(future)
        if (result == null) {
          throw new SparkException("RpcEndpoint returned null")
        }
        return result
      } catch {
        case ie: InterruptedException => throw ie
        case e: Exception =>
          lastException = e
          logWarning(s"Error sending message [message = $message] in $attempts attempts", e)
      }

      if (attempts < maxRetries) {
        Thread.sleep(retryWaitMs)
      }
    }

    throw new SparkException(
      s"Error sending message [message = $message]", lastException)
  }

```

其中的ask方法(RpcEndpointRef中是抽象的)在NettyRpcEndpointRef类中你的实现为
```
  override def ask[T: ClassTag](message: Any, timeout: RpcTimeout): Future[T] = {
    nettyEnv.ask(RequestMessage(nettyEnv.address, this, message), timeout)
  }

```

## 3.3.向BlockManagerMaster注册BlockManagerId

Executor或者Driver自身的BlockManager在初始化时,需要向Driver的BlockManager注册BlockManager信息,代码如下

```
  /** Register the BlockManager's id with the driver. */
  def registerBlockManager(
      blockManagerId: BlockManagerId, maxMemSize: Long, slaveEndpoint: RpcEndpointRef): Unit = {
    logInfo("Trying to register BlockManager")
    tell(RegisterBlockManager(blockManagerId, maxMemSize, slaveEndpoint))
    logInfo("Registered BlockManager")
  }

```

从上面的代码可以看到,小心内容包括BlockManagerId,最大内存,slaveEndpoint,消息体带有slaveEndpoint是为了便于接收driverEndpoint回复的消息,这些消息被封装为RegisterBlockManager,并调用刚刚在3.2节介绍的tell方法,根据之前的分析,RegisterBlockManager消息会被driverEndpoint匹配并执行register方法注册BlockManager,并在register方法自行结束后向发送者driverEndpoint发送一个简单的消息true(在spark1.6中没有看到,????)






# 4.磁盘块管理器DiskBlockManager

## 4.1.DiskBlockManager的构造过程
BlockManager在new的时候会创建DiskBlockManager,可以看BlockManager类,DiskBlockManager的构造步骤如下:
1.调用createLocalDirs方法创建本地文件目录,然后创建二维数组subDirs,用来缓存一级目录localDirs及二级目录




# 5.磁盘存储DiskStore


# 6.内存存储MemeoryStore

# 7.Tachyon存储TachyonStore

# 8.块管理BlockManager


# 9.metadataCleaner和broadcastCleaner

# 10.缓存管理器CacheManager

# 11.压缩算法


# 12.磁盘写入实现DiskBlockObjectWriter

# 13.块索引shuffle管理器IndexShuffleBlockManager

# 14.shuffle内存管理器ShuffleMemoryManager
