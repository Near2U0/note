

涉及的内容

* Hbase主web界面
* 使用Hbase shell管理表
* 使用Hbase shell访问Hbase中的数据
* 使用Hbase shell管理集群
* 在Hbase shell中执行Java方法
* 行计数器
* WAL工具---手动分隔和转纯纯WAL
* HFile工具---以文本方式查看HFile内容
* Hbase hbck---HBase的集群监控检查
* Hbase hive---使用类SQL语言查询Hbase中的数据


# Hbase主web界面

web界面提供了集群当前状态的概览：
* 运行hbase的版本
* Hbase的基本配置（包括hdfs的根路径和zookeeper的地址）
* 集群的平均负载以及表，region，RegionServer服务器的列表

**此外，你也可以使用某个边界行健来进行手动region的分隔，在关闭了集群的自动分隔功能时，手动分隔会非常有用**


## web界面的配置

```
c
<property>
<name>hbase.master.info.port</name>
<value>26010</value>
</property>

#regionserver的webui
<property>
<name>hbase.regionserver.info.port</name>
<value>26030</value>
</property>

```

## 两张系统表

在web首页的Tables下面有“System Tables”

![](/images/bigdata/hbase/system_tables.jpg)

这里有两张表：

hbase:meta	The hbase:meta table holds references to all User Table regions
hbase:namespace	The hbase:namespace table holds information about namespaces.

一张是hbase:meta表：存放的是所有的用户表的元数据信息（所有用户表region的引用）
一张是hbase:namespace表，表示命名空间的信息

```
hbase(main):001:0> list
TABLE                                                                                                                                     
student2                                                                                                                                  
test_table                                                                                                                                

=> ["student2", "test_table"]


hbase(main):002:0> scan 'hbase:meta'
ROW                                 COLUMN+CELL   
#hbase:namespace表                                                                                        
 hbase:namespace,,1498358802192.7e5 column=info:regioninfo, timestamp=1498358805170, value={ENCODED => 7e52e1cde895b031929335d31347efc1, N
 2e1cde895b031929335d31347efc1.     AME => 'hbase:namespace,,1498358802192.7e52e1cde895b031929335d31347efc1.', STARTKEY => '', ENDKEY => '
                                    '}                                                                                                    
 hbase:namespace,,1498358802192.7e5 column=info:seqnumDuringOpen, timestamp=1498442903374, value=\x00\x00\x00\x00\x00\x00\x00\x0C         
 2e1cde895b031929335d31347efc1.                                                                                                           
 hbase:namespace,,1498358802192.7e5 column=info:server, timestamp=1498442903374, value=hdp-node-02:16020                                  
 2e1cde895b031929335d31347efc1.                                                                                                           
 hbase:namespace,,1498358802192.7e5 column=info:serverstartcode, timestamp=1498442903374, value=1498442890842                             
 2e1cde895b031929335d31347efc1.  

#student2表                                                                                                    
 student2,,1498358933582.a066842c39 column=info:regioninfo, timestamp=1498358935794, value={ENCODED => a066842c390287b864e9c169fcd18773, N
 0287b864e9c169fcd18773.            AME => 'student2,,1498358933582.a066842c390287b864e9c169fcd18773.', STARTKEY => '', ENDKEY => ''}     
 student2,,1498358933582.a066842c39 column=info:seqnumDuringOpen, timestamp=1498443205129, value=\x00\x00\x00\x00\x00\x00\x00\x0D         
 0287b864e9c169fcd18773.                                                                                                                  
 student2,,1498358933582.a066842c39 column=info:server, timestamp=1498443205129, value=hdp-node-03:16020                                  
 0287b864e9c169fcd18773.                                                                                                                  
 student2,,1498358933582.a066842c39 column=info:serverstartcode, timestamp=1498443205129, value=1498442892611                             
 0287b864e9c169fcd18773.                                                                                                                  
 test_table,,1498366813063.2d4cd310 column=info:regioninfo, timestamp=1498366813855, value={ENCODED => 2d4cd3109c638bc7f52da8064d1db588, N
 9c638bc7f52da8064d1db588.          AME => 'test_table,,1498366813063.2d4cd3109c638bc7f52da8064d1db588.', STARTKEY => '', ENDKEY => '40000
                                    000'}   

#test_table表                                                                                  
 test_table,,1498366813063.2d4cd310 column=info:seqnumDuringOpen, timestamp=1498442903213, value=\x00\x00\x00\x00\x00\x00\x00\x05         
 9c638bc7f52da8064d1db588.                                                                                                                
 test_table,,1498366813063.2d4cd310 column=info:server, timestamp=1498442903213, value=hdp-node-02:16020                                  
 9c638bc7f52da8064d1db588.                                                                                                                
 test_table,,1498366813063.2d4cd310 column=info:serverstartcode, timestamp=1498442903213, value=1498442890842                             
 9c638bc7f52da8064d1db588.                                                                                                                
 test_table,40000000,1498366813063. column=info:regioninfo, timestamp=1498366813855, value={ENCODED => 6cec7e31c1833fa0ca85f68ef8224bcc, N
 6cec7e31c1833fa0ca85f68ef8224bcc.  AME => 'test_table,40000000,1498366813063.6cec7e31c1833fa0ca85f68ef8224bcc.', STARTKEY => '40000000', 
                                    ENDKEY => '80000000'}                                                                                 
 test_table,40000000,1498366813063. column=info:seqnumDuringOpen, timestamp=1498442903676, value=\x00\x00\x00\x00\x00\x00\x00\x05         
 6cec7e31c1833fa0ca85f68ef8224bcc.                                                                                                        
 test_table,40000000,1498366813063. column=info:server, timestamp=1498442903676, value=hdp-node-01:16020                                  
 6cec7e31c1833fa0ca85f68ef8224bcc.                                                                                                        
 test_table,40000000,1498366813063. column=info:serverstartcode, timestamp=1498442903676, value=1498442881368                             
 6cec7e31c1833fa0ca85f68ef8224bcc.                                                                                                        
 test_table,80000000,1498366813063. column=info:regioninfo, timestamp=1498366813855, value={ENCODED => a1facedf68a143cf386bb60863cbdc7c, N
 a1facedf68a143cf386bb60863cbdc7c.  AME => 'test_table,80000000,1498366813063.a1facedf68a143cf386bb60863cbdc7c.', STARTKEY => '80000000', 
                                    ENDKEY => 'c0000000'}                                                                                 
 test_table,80000000,1498366813063. column=info:seqnumDuringOpen, timestamp=1498442903675, value=\x00\x00\x00\x00\x00\x00\x00\x05         
 a1facedf68a143cf386bb60863cbdc7c.                                                                                                        
 test_table,80000000,1498366813063. column=info:server, timestamp=1498442903675, value=hdp-node-01:16020                                  
 a1facedf68a143cf386bb60863cbdc7c.                                                                                                        
 test_table,80000000,1498366813063. column=info:serverstartcode, timestamp=1498442903675, value=1498442881368                             
 a1facedf68a143cf386bb60863cbdc7c.                                                                                                        
 test_table,c0000000,1498366813063. column=info:regioninfo, timestamp=1498366813855, value={ENCODED => d73d2f4fc196673d981dd5c92e658271, N
 d73d2f4fc196673d981dd5c92e658271.  AME => 'test_table,c0000000,1498366813063.d73d2f4fc196673d981dd5c92e658271.', STARTKEY => 'c0000000', 
                                    ENDKEY => ''}                                                                                         
 test_table,c0000000,1498366813063. column=info:seqnumDuringOpen, timestamp=1498442903878, value=\x00\x00\x00\x00\x00\x00\x00\x05         
 d73d2f4fc196673d981dd5c92e658271.                                                                                                        
 test_table,c0000000,1498366813063. column=info:server, timestamp=1498442903878, value=hdp-node-03:16020                                  
 d73d2f4fc196673d981dd5c92e658271.                                                                                                        
 test_table,c0000000,1498366813063. column=info:serverstartcode, timestamp=1498442903878, value=1498442892611                             
 d73d2f4fc196673d981dd5c92e658271.                                                                                                        
6 row(s) in 0.2670 seconds

hbase(main):003:0> 
```

# 使用Hbase shell管理表


下面是一些常见的hbase shell命令

hbase shell 命令会使用Hbase配置文件（hbase-site.xml)来确定客户端将连接到哪个集群上，你也可以使用--config选项来指定Hbase shell使用另外的一个配置文件

hbase shell --config <config_directory> shell

```
# 创建表
create 't1','f1'

#列出所有的表
list

#使用describe命令来显示该表的属性
describe 't1'

#禁用表
disable 't1'

is_enabled 't1'

#修改表的属性(需要先禁用表)：下面的代码会将f1修改为只有一个版本，然后添加了一个新列族f2
alter 't1',{NAME=>'f1',VERSION='1'},{'NAME'=>'f2'}



#启用表
enable 't1'


#删除表（删除之前需要禁用表）
disable 't1'
drop 't1'

```


可以通过help命令来查看帮助

```

hbase(main):003:0> 
hbase(main):003:0> help
hbase(main):003:0> help 'create'
hbase(main):003:0> help 'drop'
hbase(main):003:0> help 'alter'

```


# 使用Hbase shell访问Hbase中的数据


hbase shell还提供了一组数据操作语言：count，delete，deleteall，get，get_counter,incr,put,scan,truncate


hbase有一个名为计数器的功能，在构建Hbase上的亮度收集系统时，他十分有用，get_counter和incr是两个用于操作计数器的命令

对大数据量的Hbase数据执行count，scan和truncate命令可能要花费较长的时间

若要统计一张大表的记录数，应该使用Hbase自带的rowcounter MapReduce任务

下面是一些DML操作
```
#添加
put 't1','row1','f1:c1','vaule1'

#获取行数
count 't1'


#全表扫描，要对记录很多的表执行scan时，不要忘记指定limit属性
scan  't1',{LIMIT=>10}


#取某一行
get 't1','row1'

#使用delete命令来删除某一指定单元格
delete 't1','row1','f1:c1'

#deleteall删除一整行（即指定行的所有单元格）
deleteall 't1','row1'


#使用incr命令让计数器的值加1
hbase(main):022:0> scan 'test_table'
ROW                                 COLUMN+CELL                                                                                           
 row_key1                           column=n:c1, timestamp=1498445786094, value=1                                                         
 row_key1                           column=n:c2, timestamp=1498445954583, value=1                                                         
1 row(s) in 0.1420 seconds

#这里直接使用incr去添加了一个列族中的列（cell）
hbase(main):023:0> incr 'test_table','row_key1','n:c3',1
COUNTER VALUE = 1

hbase(main):024:0> scan 'test_table'
ROW                                 COLUMN+CELL                                                                                           
 row_key1                           column=n:c1, timestamp=1498445786094, value=1                                                         
 row_key1                           column=n:c2, timestamp=1498445954583, value=1                                                         
 row_key1                           column=n:c3, timestamp=1498446123145, value=\x00\x00\x00\x00\x00\x00\x00\x01                          

hbase(main):025:0> incr 'test_table','row_key1','n:c3',1
COUNTER VALUE = 2
0 row(s) in 0.0350 seconds

hbase(main):026:0> incr 'test_table','row_key1','n:c3',1
COUNTER VALUE = 3
0 row(s) in 0.0400 seconds

hbase(main):027:0> incr 'test_table','row_key1','n:c3',5
COUNTER VALUE = 8
0 row(s) in 0.0150 seconds


#使用了put去修改计数器 会导致后面的错误 原因是'1'会转换成Bytes.toBytes()

hbase(main):028:0> put 'test_table','row_key1','n:c3',1

hbase(main):029:0> scan 'test_table'
ROW                                 COLUMN+CELL                                                                                           
 row_key1                           column=n:c1, timestamp=1498445786094, value=1                                                         
 row_key1                           column=n:c2, timestamp=1498445954583, value=1                                                         
 row_key1                           column=n:c3, timestamp=1498446286718, value=1                                                         
1 row(s) in 0.0830 seconds

hbase(main):030:0> incr 'test_table','row_key1','n:c3',1

ERROR: org.apache.hadoop.hbase.DoNotRetryIOException: Field is not a long, it's 1 bytes wide
        at org.apache.hadoop.hbase.regionserver.HRegion.getLongValue(HRegion.java:7366)
........

＃默认步长是1
hbase(main):032:0> incr 'test_table','row_key1','n:c4',1
COUNTER VALUE = 1
 
hbase(main):033:0> incr 'test_table','row_key1','n:c4'
COUNTER VALUE = 2

＃计数器可以－1
hbase(main):034:0> incr 'test_table','row_key1','n:c4',-1
COUNTER VALUE = 1

＃计数也可为0
hbase(main):035:0> incr 'test_table','row_key1','n:c4',0
COUNTER VALUE = 1


#获取计数器
hbase(main):037:0> get_counter 'test_table','row_key1','n:c4'
COUNTER VALUE = 1


#清空表数据
hbase(main):038:0> truncate 'test_table'
Truncating 'test_table' table (it may take a while):
 - Disabling table...
 - Truncating table...
0 row(s) in 4.7440 seconds

hbase(main):039:0> scan 'test_table'
ROW                                 COLUMN+CELL                                                                                           
0 row(s) in 0.1330 seconds

hbase(main):040:0> 


#count计数行（会将行健相同的作为一行）
hbase(main):041:0> scan 'student2'
 rowkey                             column=info2:age, timestamp=1498358947433, value=22                                                   
 rowkey2                            column=info2:gender, timestamp=1498445504112, value=man                                               
2 row(s) in 0.0520 seconds

hbase(main):042:0> put 'student2','rowkey','info2:name','zhangsan'
hbase(main):043:0> scan 'student2'
 rowkey                             column=info2:age, timestamp=1498358947433, value=22                                                   
 rowkey                             column=info2:name, timestamp=1498446803138, value=zhangsan                                            
 rowkey2                            column=info2:gender, timestamp=1498445504112, value=man                                               

hbase(main):044:0> count 'student2'
=> 2


```


# 使用Hbase shell管理集群

Hbase提供了一个可用于手动管理region的接口，其功能包括：
* region部署
* region分隔
* 集群负载均衡
* region的写盘与合并



可以使用下面的命令来预创建region

```
[root@hdp-node-03 bin]# ./hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 4 -f n
```

我们首先将手动对这些region进行**写盘和合并**，然后再手动对他们进行**分隔**，最后，我们将重新对这些region进行**负载均衡**，以使它们能均衡的分布在集群之中

1.flush命令可将表中所有region区域的数据都写入磁盘

```
#插入数据
hbase(main):054:0> for i in 1..100 do put 'test_table','row_#{i}','n:cc',i end

hbase(main):052:0> count 'test_table'
1 row(s) in 0.0450 seconds
=> 1

#查看hdfs中对应的数据
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n
[root@hdp-node-02 ~]#


#flush刷写磁盘
hbase(main):053:0> flush 'test_table'
0 row(s) in 0.6670 seconds


#再次查看hdfs
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n
-rw-r--r--   3 root supergroup       4979 2017-06-26 11:31 /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n/1d631e541df34bef9588e9be9ad0b636
[root@hdp-node-02 ~]# 


#若要单独将某个region的数据写入磁盘，只需要将该region的名字作为参数传给flush
#表的region的名字可以在webui上查看
flush 'test_table,,1498446635452.f344a8a0faa00cc9c9bb0f7d24963e9c.'

```

2.执行compact命令合并表的 hfile文件

```
#合并之前
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n
-rw-r--r--   3 root supergroup       4979 2017-06-26 11:31 /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n/1d631e541df34bef9588e9be9ad0b636
-rw-r--r--   3 root supergroup       4991 2017-06-26 11:38 /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n/6a7894378ab8488da2df4e43e80f2a81
-rw-r--r--   3 root supergroup     449647 2017-06-26 11:47 /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n/cf240e28bc164c058097c75ed0adacca

#合并
hbase(main):065:0> compact 'test_table'

#合并之后
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n
-rw-r--r--   3 root supergroup     429661 2017-06-26 11:47 /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c/n/023d0910ea17421a85d62e5bd38bbf21
[root@hdp-node-02 ~]# 

```


3.执行major_compact命令对指定表进行主合并

```
hbase(main):068:0> major_compact 'test_table'

```


4.执行split命令可以对表的指定region进行分隔

拆分之前

```
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table
drwxr-xr-x   - root supergroup          0 2017-06-26 11:10 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-26 11:10 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-06-26 11:31 /hbase/data/default/test_table/f344a8a0faa00cc9c9bb0f7d24963e9c
[root@hdp-node-02 ~]# 

```
![](/images/bigdata/hbase/split_1.jpg)


拆分之后
```
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table
drwxr-xr-x   - root supergroup          0 2017-06-26 11:10 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-26 11:10 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-06-26 11:51 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6
drwxr-xr-x   - root supergroup          0 2017-06-26 11:51 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb
[root@hdp-node-02 ~]# 

```

![](/images/bigdata/hbase/split_2.jpg)


发现一个region已经被拆成了两个region


5.负载均衡


* 可以使用balance_switch命令来启用负载均衡功能

```
hbase>balance_switch true
true
```

输出结构（true）显示的是执行该命令之前的负载均衡状态（均衡）
如果是false，那么表示执行该命令之前的负载的状态是不均衡

* 使用balancer命令来对集群进行负载均衡
```
hbase>balancer
true
```

输入结果（true）表示负载均衡已被成功触发，他会在主服务器的后台进行运行

6.使用move命令可以将某一区域移动到指定regionServer服务器上

可以看到“test_table”拥有的region，以及对应的regionServer

![](/images/bigdata/hbase/move_region.jpg)

在hdfs中可以看到
```
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/

drwxr-xr-x   - root supergroup          0 2017-06-26 11:10 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-26 11:10 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-06-26 11:51 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6
drwxr-xr-x   - root supergroup          0 2017-06-26 11:51 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb

```

移动指定的region(a23e6a902233c23c1c7846c953b301e6)到指定的regionServer上
```
hbase(main):023:0> move 'a23e6a902233c23c1c7846c953b301e6','hdp-node-03,16020,1498442892611'

```

再次查看结果

![](/images/bigdata/hbase/move_region2.jpg)




* 运行原理

1.在hbase发生数据修改的时候，数据修改会先被写入预写日志（WAL）中，然后再被写入region服务器的MemStore，在MemStore的大小达到一定的阈值之前，该修改不会被写到磁盘上，此阈值默认是128M，可以在hbase-site.xml文件中修改（hbase.hregion.memstore.flush.size），flush命令会同步的将这些待写入的记录写入到磁盘中。该命名需要以一个表名或区域名为参数，你可以在该表的web页面下的Table Regions中找到各region的名字

2.如果传给compact和major_compact命令的是一个region名（而不是一个表名），那么就将指定的region进行合并，compact和major_compact是异步命令，执行这两个命令会把所指定的表或region加入到合并队列中，合并操作将在所指定的region所在的服务器上以后台运行的方式执行，**手动major_compact应当选择在低负载时段进行，这是因为合并不仅会产生大量的磁盘IO和网络通信，而且需要很长时间才能完成**

3.我们可以使用split命令来对某张表的某个指定region进行分隔，如果hbase运行时的写负载很高，那么就应该关闭hbase的自动region分隔功能，然后使用自定义的算法来预创建一些 区域，并且在某些区域增长到过大时使用split命令来对这些区域进行分隔


4.如果数据一直在持续增长，集群就有可能变得不均衡，为了使集群的负载均衡，hbase会定期在后台运行负载均衡程序，另外你可以使用balance_switch命令来显式的启用负载均衡功能，然后再使用balancer命令来触发一个负载均衡操作，当某个regionServer需要停机进行维护的时候，关闭负载均衡程序也非常有用


5.默认的均衡算法只是简单的让每个regionServer都部署更多的region，如果该算法不适合你的数据访问模式，那么你可以使用move命令来手动将某一region移到指定的regionServer上，move需要以编码的region名称和目标服务器名称为参数，以编码的region名称是region名称中的那段哈希码后缀，如下

![](/images/bigdata/hbase/split_3.jpg)


目标服务器的格式是：hostname:port:start-code

![](/images/bigdata/hbase/target_regionServer.jpg)

调用move命令会首先在其最初部署的服务器上指定region关闭，然后在目标服务器上将该region打开，最后再在hbase:meta表中更新该region的记录



# 在Hbase shell中执行Java方法

本节将展示2个例子

* 1.将hbase shell输出结果中的时间戳转换为一种具有可读性的日期格式
* 2.导入一个hbase的过滤器类，然后在执行scan命令时使用该过滤器

操作步骤如下：

```
hbase(main):050:0> scan 'student2'
ROW                                 COLUMN+CELL                                                                                           
 rowkey                             column=info2:age, timestamp=1498358947433, value=22                                                   
 rowkey                             column=info2:name, timestamp=1498459540968, value=zhangsan33333333222222                              
 rowkey2                            column=info2:gender, timestamp=1498445504112, value=man                                               
 rowkey3                            column=info2:name, timestamp=1498459549547, value=zhangsan33333333222222                              
 rowkey34                           column=info2:name, timestamp=1498459555303, value=zhangsan33333333222222                              

#调用Date类
hbase(main):032:0> import java.util.Date
hbase(main):032:0> Date.new(1498450137175).toString()
=> "Mon Jun 26 12:08:57 CST 2017"


#导入对应的类
hbase(main):047:0> import org.apache.hadoop.hbase.filter.PrefixFilter
hbase(main):048:0> import org.apache.hadoop.hbase.util.Bytes

#在filter中使用过滤器（PrefixFilter）
hbase(main):051:0> scan 'student2',{FILTER=>PrefixFilter.new(Bytes.toBytes('rowkey3')), COLUMN=>'info2:name'}
ROW                                 COLUMN+CELL                                                                                           
 rowkey3                            column=info2:name, timestamp=1498459549547, value=zhangsan33333333222222                              
 rowkey34                           column=info2:name, timestamp=1498459555303, value=zhangsan33333333222222                              
#会过滤行健中包含指定前缀的行

```


hbase还附带另外的许多种有用的Java类，如下

```
org.apache.hadoop.hbase.util
org.apache.hadoop.hbase.client
org.apache.hadoop.hbase.filter
org.apache.hadoop.hbase.mapreduce

```



# 行计数器


hbase shell中的count命令是统计表行数的简单方法，但是如果表中的数据量巨大，count命令就可能需要经过很长时间才能返回结构，这种情况更适合使用RowCounter类来进行行统计，该类会启动一个MapReduce任务来统计表的行数，因此他的效率要比count命令高很多


```
$HBASE_HOME/bin/hbase   org.apache.hadoop.hbase.mapreduce.RowCounter 'test_table' 


[root@hdp-node-02 ~]# /bigdata_installed/hbase/bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter 'test_table' 

#打印如下的结果
        org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters
                ROWS=80438
        File Input Format Counters 
                Bytes Read=0
        File Output Format Counters 
                Bytes Written=0


#而是使用count的执行结果如下 
hbase(main):052:0> count 'test_table'
=> 80438

```


# WAL工具---手动分隔和转存储WAL

hbase的每个数据修改操作都会首先被写入到region服务器的预写日志中（WAL),该修改操作在成功写入日志之后，继而更新到regionServer的MemStore中，WAL是hdfs上的一个顺序文件，默认情况下他会被自动复制到另外两台DataNode服务器上，因此单台regionServer的崩溃不会导致该服务器上的数据丢失

因为一个regionServer上的所有region都会共享一个WAL，所以为了恢复某台发生崩溃的regionServer，首先要将该regionServer上的WAL进行拆分，然后在每个相关region上重新执行WAL,Hbase会使用这种算法自动处理regionServer服务器的故障转移操作

hbase带有一个WAL工具，可供我们手动对WAL进行分隔和转存


准备数据
```
hbase(main):059:0> scan 'student2'
0 row(s) in 0.1370 seconds

hbase(main):060:0> put 'student2','rowkey','info2:name','zhangsan'
hbase(main):061:0> put 'student2','rowkey','info2:age',22
hbase(main):062:0> put 'student2','rowkey','info2:addr','beijing'
hbase(main):063:0> delete 'student2','rowkey','info2:addr'

hbase(main):064:0> scan 'student2'
ROW                                 COLUMN+CELL                                                                                           
 rowkey                             column=info2:age, timestamp=1498463035295, value=22                                                   
 rowkey                             column=info2:name, timestamp=1498463019890, value=zhangsan                                            


```

./hbase org.apache.hadoop.hbase.io.hfile.HFile -p -f /hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2/542cc6c528cc4e82895e9992ba05cdd3

运行原理说明

hbase的数据修改在内部要用org.apache.hadoop.hbase.KeyValue类来实现，当一台regionServer服务器接收到一个数据修改时，他就会将其（一个KeyValue实例）追加到WAL文件中，然后再真正的更新其在MemStore中的对应数据，只有在MemStore的大小达到某一阈值（默认是128M）时，这些数据的修改才会刷写到磁盘（hfile）中，从而持久化保存在hdfs中

待整理：
http://www.itdadao.com/articles/c15a597264p0.html
http://blog.csdn.net/qiruiduni/article/details/50434373
http://blog.csdn.net/qiruiduni/article/details/50437283
http://blog.csdn.net/chicm/article/details/40952563



# HFile工具---以文本方式查看HFile内容

Hfile是Hbase内部存储数据的文件格式，在其源码中是这样介绍Hfile的：

* Hbase的文件格式
* 一种排序“键值对”的文件，键和值均为字节数组

下面将介绍如何使用hfile工具来以文本方式显示hfile文件的内容和元数据

## 数据准备

hfile在hdfs中的路径在：/hbase/data/default/<tableName>/region_name/column_family目录下面

```
[root@hdp-node-02 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls  /hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2
-rw-r--r--   3 root supergroup       5068 2017-06-26 16:43 /hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2/542cc6c528cc4e82895e9992ba05cdd3

```


## 操作

1.以文本的方式显示hfile文件中的键值对的内容

```
[root@hdp-node-02 bin]# ./hbase org.apache.hadoop.hbase.io.hfile.HFile -p -f /hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2/542cc6c528cc4e82895e9992ba05cdd3

2017-06-26 17:51:32,395 INFO  [main] hfile.CacheConfig: CacheConfig:disabled
K: rowkey/info2:addr/1498463095548/DeleteColumn/vlen=0/seqid=12 V: 
K: rowkey/info2:age/1498463035295/Put/vlen=2/seqid=8 V: 22
K: rowkey/info2:name/1498463019890/Put/vlen=8/seqid=6 V: zhangsan
Scanned kv count -> 3
[root@hdp-node-02 bin]# 

```
我们可以看到hfile中确实是键值对的方式
如第一条数据：key=rowkey/info2:addr/1498463095548/DeleteColumn/vlen=0/seqid=12; value=null
第二条数据：key=rowkey/info2:age/1498463035295/Put/vlen=2/seqid=8 ; value=22


2.如果要显示hfile文件的元数据，在使用hfile工具时必须带上-m选项

```
[root@hdp-node-02 bin]# ./hbase org.apache.hadoop.hbase.io.hfile.HFile -p -m -f /hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2/542cc6c528cc4e82895e9992ba05cdd3

2017-06-26 17:55:50,932 INFO  [main] hfile.CacheConfig: CacheConfig:disabled
K: rowkey/info2:addr/1498463095548/DeleteColumn/vlen=0/seqid=12 V: 
K: rowkey/info2:age/1498463035295/Put/vlen=2/seqid=8 V: 22
K: rowkey/info2:name/1498463019890/Put/vlen=8/seqid=6 V: zhangsan

Block index size as per heapsize: 400
reader=/hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2/542cc6c528cc4e82895e9992ba05cdd3,
    compression=none,
    cacheConf=CacheConfig:disabled,
    firstKey=rowkey/info2:addr/1498463095548/DeleteColumn,
    lastKey=rowkey/info2:name/1498463019890/Put,
    avgKeyLen=26,		#键平均长度
    avgValueLen=3,		#值平均长度
    entries=3,			#entries表示该region中数据的数量
    length=5068
Trailer:
    fileinfoOffset=313,
    loadOnOpenDataOffset=199,
    dataIndexCount=1,
    metaIndexCount=0,
    totalUncomressedBytes=4971,
    entryCount=3,
    compressionCodec=NONE,
    uncompressedDataIndexSize=40,
    numDataIndexLevels=1,
    firstDataBlockOffset=0,
    lastDataBlockOffset=0,
    comparatorClassName=org.apache.hadoop.hbase.KeyValue$KeyComparator,
    encryptionKey=NONE,
    majorVersion=3,		#进行了多少次major_compact
    minorVersion=0
Fileinfo:
    BLOOM_FILTER_TYPE = ROW
    DELETE_FAMILY_COUNT = \x00\x00\x00\x00\x00\x00\x00\x00
    EARLIEST_PUT_TS = \x00\x00\x01\x5C\xE3[#r
    KEY_VALUE_VERSION = \x00\x00\x00\x01
    LAST_BLOOM_KEY = rowkey
    MAJOR_COMPACTION_KEY = \x00
    MAX_MEMSTORE_TS_KEY = \x00\x00\x00\x00\x00\x00\x00\x0C
    MAX_SEQ_ID_KEY = 13
    TIMERANGE = 0....1498463095548
    hfile.AVG_KEY_LEN = 26		#键平均长度
    hfile.AVG_VALUE_LEN = 3		#值平均长度
    hfile.CREATE_TIME_TS = \x00\x00\x01\x5C\xE3\x921\xA0
    hfile.LASTKEY = \x00\x06rowkey\x05info2name\x00\x00\x01\x5C\xE3[#r\x04
    hfile.MAX_TAGS_LEN = \x00\x00\x00\x00
    hfile.TAGS_COMPRESSED = \x00
Mid-key: \x00\x06rowkey\x05info2addr\x00\x00\x01\x5C\xE3\x5CJ\xFC\x0C
Bloom filter:
    BloomSize: 2
    No of Keys in bloom: 1
    Max Keys for bloom: 1
    Percentage filled: 100%
    Number of chunks: 1
    Comparator: RawBytesComparator
Delete Family Bloom filter:
    Not present
Scanned kv count -> 3
[root@hdp-node-02 bin]# 

```

或者直接查看region(-r后面接region的名称)
```
[root@hdp-node-02 bin]# ./hbase org.apache.hadoop.hbase.io.hfile.HFile -m -r student2,,1498462972683.e2ac10cef6bca829b02843ca96cf36f7.
2017-06-26 18:14:53,832 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-06-26 18:14:54,999 INFO  [main] hfile.CacheConfig: CacheConfig:disabled
Block index size as per heapsize: 400
reader=hdfs://hdp-node-01:9000/hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2/4c55d2c08ec5462e92cf9c9885ba4005,
    compression=none,
    cacheConf=CacheConfig:disabled,
    firstKey=rowkey/info2:age/1498463035295/Put,
    lastKey=rowkey/info2:name/1498463019890/Put,
    avgKeyLen=27,
    avgValueLen=4,
    entries=3,
    length=5010
Trailer:
    fileinfoOffset=311,
    loadOnOpenDataOffset=198,
    dataIndexCount=1,
    metaIndexCount=0,
    totalUncomressedBytes=4914,
    entryCount=3,
    compressionCodec=NONE,
    uncompressedDataIndexSize=39,
    numDataIndexLevels=1,
    firstDataBlockOffset=0,
    lastDataBlockOffset=0,
    comparatorClassName=org.apache.hadoop.hbase.KeyValue$KeyComparator,
    encryptionKey=NONE,
    majorVersion=3,
    minorVersion=0
Fileinfo:
    BLOOM_FILTER_TYPE = ROW
    DELETE_FAMILY_COUNT = \x00\x00\x00\x00\x00\x00\x00\x00
    EARLIEST_PUT_TS = \x00\x00\x01\x5C\xE3[#r
    KEY_VALUE_VERSION = \x00\x00\x00\x01
    LAST_BLOOM_KEY = rowkey
    MAJOR_COMPACTION_KEY = \xFF
    MAX_MEMSTORE_TS_KEY = \x00\x00\x00\x00\x00\x00\x00\x12
    MAX_SEQ_ID_KEY = 19
    TIMERANGE = 1498463019890....1498470529792
    hfile.AVG_KEY_LEN = 27
    hfile.AVG_VALUE_LEN = 4
    hfile.CREATE_TIME_TS = \x00\x00\x01\x5C\xE3\xDB\xDDT
    hfile.LASTKEY = \x00\x06rowkey\x05info2name\x00\x00\x01\x5C\xE3[#r\x04
Mid-key: \x00\x06rowkey\x05info2age\x00\x00\x01\x5C\xE3[_\x9F\x04
Bloom filter:
    BloomSize: 2
    No of Keys in bloom: 1
    Max Keys for bloom: 1
    Percentage filled: 100%
    Number of chunks: 1
    Comparator: RawBytesComparator
Delete Family Bloom filter:
    Not present
[root@hdp-node-02 bin]# 
```


这些元数据信息非常有用，其中包括：块索引大小，键值对的平均长度

3.使用如下命令来获取指定region中的数据量(该region的总条目数）
```
[root@hdp-node-02 bin]# ./hbase org.apache.hadoop.hbase.io.hfile.HFile -p -m -f /hbase/data/default/student2/e2ac10cef6bca829b02843ca96cf36f7/info2/542cc6c528cc4e82895e9992ba05cdd3|grep entries

2017-06-26 17:58:17,896 INFO  [main] hfile.CacheConfig: CacheConfig:disabled
    entries=3,
[root@hdp-node-02 bin]# 
```

Hfile工具只是扫描指定Hfile文件中的所有数据块，然后就可以将所有键值对打印到输出中，请注意，哪些被删除的单元格在hfile文件中仍有相应的条目，只有对该region执行了major_compact之后，该条目才会消失


参数说明

```
[root@hdp-node-02 bin]# ./hbase org.apache.hadoop.hbase.io.hfile.HFile
usage: HFile [-a] [-b] [-e] [-f <arg> | -r <arg>] [-h] [-k] [-m] [-p] [-s] [-v] [-w <arg>]
 -a,--checkfamily         Enable family check
 -b,--printblocks         Print block index meta data
 -e,--printkey            Print keys
 -f,--file <arg>          File to scan. Pass full-path; e.g.			后接文件路径
                          hdfs://a:9000/hbase/hbase:meta/12/34
 -h,--printblockheaders   Print block headers for each block.
 -k,--checkrow            Enable row order check; looks for out-of-order
                          keys
 -m,--printmeta           Print meta data of file					输出一些元数据信息
 -p,--printkv             Print key/value pairs						告诉命令要将键值对的内容包含在输出结果中
 -r,--region <arg>        Region to scan. Pass region name; e.g.		后接region的名称v
                          'hbase:meta,,1'
 -s,--stats               Print statistics
 -v,--verbose             Verbose output; emits file and meta data
                          delimiters
 -w,--seekToRow <arg>     Seek to this row and print all the kvs for this
                          row only
[root@hdp-node-02 bin]# 

```

# Hbase hbck---HBase的集群监控检查

hbase提供了hbck命令来检查各种不一致问题，下面的介绍是出自hbck的源程序

> 检查数据在master及regionServer的内存中状态与数据在hdfs中的状态之间的一致性


hbase的hbck不仅能够检查不一致问题，而且还能够修复不一致问题

下面我们通过在集群中制造一些不一致的情况，然后演示如何使用hbck来解决问题

## 操作步骤

```
[root@hdp-node-02 ~]# /bigdata_installed/hbase/bin/hbase hbck 

#当集群出现不一致的状态时，会出现下面的语句：
Status: INCONSISTENT


#可以使用-fix进行简单的修复
/bigdata_installed/hbase/bin/hbase hbck -fix

```


* Hbase hive---使用类SQL语言查询Hbase中的数据

未整理












