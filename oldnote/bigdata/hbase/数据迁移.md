
# 数据导入的几种方式

将数据移到Hbase的方法有以下几种：

* 使用Hbase的Put API 
* 使用Hbase的批量加载工具
* 使用自定义的MapReduce任务


1.对于Put方式，特别是一大批数据需要移入Hbase并且对移入时间有限定的情况下，这种方法的效率并不高

2.批量导入的功能支持高效的将大量数据加载到Hbase中，批量加载功能使用了一个MapReduce任务将数据加载到一个特定的Hbase表中，他会生成一些Hbase内部的HFile数据格式的文件，然后再将这些数据文件直接加载到正在运行的集群中，这里hbase提供了一个导入的工具：importtsv（他会运行一些MapReduce任务来读取TSV文件中的数据，然后将其输出直接写入Hbase表或者Hbase内部数据格式的文件中）

3.对于一些其他格式的数据，需要手动编写MapReduce来进行导入


# 导入mysql数据

数据迁移最常见的就是将RDBMS中的数据导入到hbase中，对于数据量不大的情况，我们可以：用一个客户端程序来读取数据，然后通过hbase的Put API把数据送到hbase中

这里只是说实现的步骤：
1.在hbase shell中建表
2.连接mysql，读取数据
3.将读取到的数据通过Put的方式写入到hbase中
4.在hbase shell中查看数据是否导入成功



# 使用批量加载工具导入TSV文件的数据

hbase的一个自带的工具importtsv，他支持将文件中的数据导入到hbase中，使用此工具将文件数据加载到hbase的效率非常高，因为他会运行一个MapReduce任务来进行导入，即便你需要加载的数据来来自RDBMS，你也可以先将数据转存到某种方式的一个文件文件中，然后再使用importtsv来讲转存的数据导入到hbase中

importtsv工具不仅能够将数据加载到hbase表中，还能生成hbase内部格式的文件（hfile），因此你可以使用hbase的批量加载工具将所生成的这些文件直接加载到一个正在运行的hbase集群中，这种方式可以减少迁移过程中因数据传输而产生的网络流量和给hbase带来的负载

实现步骤：
1.将tsv文件导入的hdfs中
2.在hbase shell中建表
3.执行导入操作（importtsv）
4.在hbase shell中检查结果

具体参见:[bulk load导入数据到hbase中.md](http://ww)

该工具会为我们启动一个MapReduce任务，在map阶段，该任务会读取并解析指定输入目录下的tsv文件的行，然后根据列映射信息将这些行插入到hbase中，**他会在多台服务器上并行执行Read和Put操作**(因为数据是通过Put插入的，那么数据首先是写到HLog，然后是metastore，再然后是hfile中)，所以速度要比从一个客户端加载数据要快很多，该任务在默认情况下没有reduce阶段


补充说明：

在默认情况下，importtsv工具在其map阶段会使用TableOutputFormat类来通过Hbase的Put API将数据插入到hbase表中的，但是如果指定了-Dimporttsv.bulk.output选项，他就会转而使用HFileOutputFormat类来在hdfs上生成一些hbase内格式（hfile）的文件，这时，我们就可以使用completebulkload工具将所生成的文件加载到一个正在运行的集群中，使用批量输出和加载的工具如下：

```
./hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles hdfs://hdp-node-01:9000/storefile-outputdir student
```

completebulkload工具会查看所生成的这些文件，确定他们所属的regionServer，然后连接regionServer，regionServer会将接收到的hfile文件移动到其存储目录中，然后以联机的方式为客户端创建这一数据



# 编写自定义的MapReduce任务来导入数据

在将文本文件加载到Hbase中时，importtsv工具非常有用，但是在许多情况下，我们需要对加载的过程进行全面控制，这时可能就需要编写一个自己的MapReduce任务来将数据导入到Hbase中，举例来说，如果要加载的数据是一些其他格式的文件，importtsv工具就不能胜任了

Hbase提供了一个TableOutputFormat类，我们可以在MapReduce任务中使用它来讲数据写入到一张Hbase的表中，我们还可以在MapReduce任务中使用HFileOutputFormat类来生成一些Hbase内部格式的文件（hfile），然后使用completebulkload工具将所生成的这些文件加载到一个正在运行的Hbase集群中


使用TableOutputFormat类是将数据直接写入到Hbase表中，这种方式还是不够高效，在进行数据写入的时候，不是直接写入数据到hbase中，而是在自己的mapreduce任务中生成一些内部hfile格式的文件，然后使用completebulkload工具将这些文件加载到集群中，与简单的使用TableOutputFormat API的方式相比，这种方法所使用的CPU资源和网络资源更少


# 影响数据迁移的一些重要配置

如果在MapReduce任务中使用TableOutputFormat直接将数据写入到Hbase表中，那么hbase上的写操作可能会非常的繁重，虽然hbase在设计上能够迅速的处理些操作，但是你可能仍然需要对下列这些配置参数进行一些调整：

* jvm堆和gc参数
* 区域服务器（regionServer）的处理进程数
* 区域文件的最大的尺寸
* memstore的大小
* 更新块的设置



# 在数据移入hbase前预创建region

hbase中的每一行都隶属于某一个特定的区域（region)，在一个区域中，保存有某一范围内的已经排好序的hbase的记录，region由服务器来部署和管理

当我们在hbase中创建一张表的时候，该表会自动创建一个region，插入该表的所有数据会首先进入到这个region中，随着表中的数据不断的插入，当数据量到达一定阈值时，该region就会分解成两半（这就是所谓的region spliting),分隔出来的region会分布到其他regionServer上，因此负载可以在集群之中得到均衡分配

正如你想的那样，**如果在初始化表时就使用某种合适的算法来预先创建好一些region，那么数据迁移的负载就可以均衡的分布到整个集群中**，这样就能显著的提高数据加载的速度

操作步骤
```
[root@hdp-node-03 bin]# ./hbase org.apache.hadoop.hbase.util.RegionSplitter test_table HexStringSplit -c 4 -f n
```
参数说明
```
-c 4 创建表的同时预创建4个region
-f n 创建一个名为n的列族
test_table 表名
HexStringSplit：Hbase自带了两种pre-split的算法，分别是 HexStringSplit 和  UniformSplit 。如果我们的row key是十六进制的字符串作为前缀的，就比较适合用HexStringSplit，作为pre-split的算法。例如，我们使用HexHash(prefix)作为row key的前缀，其中Hexhash为最终得到十六进制字符串的hash算法。我们也可以用我们自己的split算法。


```

在hdfs中查看结果如下：可以看到创建的表中有4个region，列族为n

```
[root@hdp-node-02 bin]# ./hdfs dfs -ls /hbase/data/default/test_table

drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/2d4cd3109c638bc7f52da8064d1db588
drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/6cec7e31c1833fa0ca85f68ef8224bcc
drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/a1facedf68a143cf386bb60863cbdc7c
drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/d73d2f4fc196673d981dd5c92e658271


#查看某一个region的列族
[root@hdp-node-02 bin]# ./hdfs dfs -ls /hbase/data/default/test_table/2d*

-rw-r--r--   3 root supergroup         53 2017-06-25 13:00 /hbase/data/default/test_table/2d4cd3109c638bc7f52da8064d1db588/.regioninfo
drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/2d4cd3109c638bc7f52da8064d1db588/n
drwxr-xr-x   - root supergroup          0 2017-06-25 13:00 /hbase/data/default/test_table/2d4cd3109c638bc7f52da8064d1db588/recovered.edits
[root@hdp-node-02 bin]# 
```

参见：http://www.cnblogs.com/niurougan/p/3976519.html


RegionSplitter是hbase提供的一个工具类，你可以使用RegionSplitter来：

* 创建一张带有指定数量预先创建region的表
* 对已有表中的所有区域执行滚动分隔
* 使用自定义的算法进行region分隔

所以通过直接插入数据的到hbase表的情况，我们可以看到数据是写入到了一个region中，即执行任务期间的所有的请求发送到了同一台服务器

出现这种情况的原因是；默认分隔算法（MD5StringSplit）不适合我们的情况，按照这种分隔算法，我们的所有行都都落在了一个region中，所以为了能正确的进行区域分隔，我们需要提供一个自定义的分隔算法

对于那些要生成内部hfile格式文件的MapReduce任务来说，预分隔region也会改变任务的执行方式，当我们再次生成运行hfile的程序的时候，我们发现，MapReduce的reduce计数会从原来的1跳到4，而4正好是预先创建的区域的个数

这是因为：任务的reduce进程数是由目标表的region个数所决定的，如果reduce进程数增加了，通常都意味着被分布到多台服务器上，因此任务的执行速度就会快很多





























