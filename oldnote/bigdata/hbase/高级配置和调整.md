# 增加RegionServer的处理线程数

为了防止RegionServer在运行时耗尽内存，该线程数默认都设置的非常低，在很多情况下，尤其是在有大量并发客户端的时候，为了处理更多的请求，你需要将该数值调高

## 操作步骤

1.在主节点的hbase-site.xml中添加如下

```
<property>
<name>hbase.regionserver.handler.count</name>
<value>40</value>
</property>
```

2.将改动同步到整个集群中

```
for slave in `cat $hbase_home/conf/regionservers`;do
	rsync -avz $hbase_home/conf/hbase-site.xml $slave:$hbase_home/conf/hbase-site.xml
done
```

3.重启hbase使修改生效


# 三、使用自定义算法预创建区域

当我们在HBase中创建一张表时，该表一开始只有一个区域。插入该表的所有数据会保存在这个区域中。随着数据的不断增加，当该区域的大小达到一定阀值时，就会发生区域分割（Region Splitting）。这个区域会分成两半，以便使该表可以处理更多的数据。

在写密集的HBase集群中，区域分割会带来以下几个需要解决的问题。
* 分割/合并风暴问题。
如果数据均匀地增长，大部分区域就会在同一时间发生分割，从而导致大量的磁盘I/O和网络流量。
* 只有在分割出了足够多个区域之后，负载才会均衡。
尤其是在表刚创建时，所有请求都会发给首个区域所在的那台RegionServer。


在默认情况下，RegionSplitter实用工具会使用MD5算法来生成MD5校验和，以此来作为区域的开始键。键值的范围是“00000000”到“7FFFFFFF”。这种算法（HexStringSplit）适用于很多情况，但在有些情况下，你可能更需要使用一种自己的算法来生成键值，以使负载能在集群中分散开来。
HBase的行键完全由向HBase写入数据的应用程序所控制，在很多情况下，（由于这样或那样的原因）行键的范围和分布情况是可以预测的。因此，可以先计算出各区域的分割键，然后再使用这些分割键来创建预分割区域。
我们将使用一组记录在文本文件中的区域开始键来创建表的预定义区域。
1.创建一个split-keys文件。在该文件中输入一组区域分割键，每个键一行。假设该文件包含有如下一些开始键：
$ cat split-keys
```
a0000
affff
b0000
bffff
```

2.创建Java类FileSplit，用它来实现org.apache.hadoop.hbase.util.RegionSplitter.SplitAlgorithm接口。

```
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.util.RegionSplitter.SplitAlgorithm;

public class FileSplit implements SplitAlgorithm {
//	public static final String SPLIT_KEY_FILE = "split-keys";
	private String splitKeyFile;

	public FileSplit (String splitKeyFile) {
		this.splitKeyFile = splitKeyFile;
	}
	
	@Override
	public byte[] split(byte[] start, byte[] end) {
		return null;
	}

	@Override
	public byte[][] split(int numberOfSplits) {
		BufferedReader reader=null;
		try {
			List<byte[]> regions = new ArrayList<byte[]>();
			//一行一行读
			reader = new BufferedReader(new FileReader(splitKeyFile));
			String line;
			while ((line = reader.readLine()) != null) {
//				System.out.println(line);		
				if (line.trim().length() > 0) {
					regions.add(Bytes.toBytes(line));
				}
			}
			return regions.toArray(new byte[0][]);
		} catch (IOException e) {
			throw new RuntimeException("Error reading splitting keys from " + splitKeyFile, e);
		} catch (Exception e) {
			e.printStackTrace();
		} finally {
			try {
				if (reader != null) reader.close();
			} catch (IOException e) {
				e.printStackTrace();
			}
		}
		return null;
	}

	@Override
	public byte[] firstRow() {
		return null;
	}

	@Override
	public byte[] lastRow() {
		return null;
	}

	@Override
	public byte[] strToRow(String input) {
		return null;
	}

	@Override
	public String rowToStr(byte[] row) {
		return null;
	}

	@Override
	public String separator() {
		return null;
	}

}
```

3.编译该Java文件。
```
$ javac -classpath $HBASE_HOME/hbase-0.92.0.jar FileSplit.java
```
4.将包含有分割键信息的split-keys文件复制到编译生成FileSplit类的目录下。
5.运行如下脚本来在创建表的时候预创建一些区域。
```
$ export HBASE_CLASSPATH=$HBASE_CLASSPATH:./
$ $HBASE_HOME/bin/hbase org.apache.hadoop.hbase.util.RegionSplitter -D split.algorithm=FileSplit -c 2 -f f1 test_table
```
6.通过HBase的Web用户界面确认表和预定义区域是否已正确创建。
HBase带有一个名为RegionSplitter的实用工具，该类可以用于：
* 创建带预分割区域的HBase表
* 对现有的表中的所有区域进行滚动分割
* 使用自定义算法来分割区域

为了能在创建表时分割区域，只需要像第2步那样实现一下split()方法。RegionSplitter类会调用该方法来对整张表进行分割。在我们的实现中，该方法会从我们准备好的文件中读取分割键（每个键一行），然后将它们转换并保存在一个存储该表所有初始区域的分割键的byte[]数组中。
为了能使用该类来分割区域，我们将FileSplit类加入到了HBASE_CLASSPATH中，然后以带如下参数的方式调用了RegionSplitter实用程序。

* -D split.algorithm=FileSplit：指定分割算法。
* -c 2：表分割的区域数。在本实现中没有用到。
* -f f1：创建一个名为“f1”的列族。
* test_table：待创建的表的名称。

正如你在第6步中看到那样，我们用4个分割键将test_table分成了5个区域。这4个分割键正是我们写在split-keys文件中的内容。
该表的其他属性都使用了默认值，你可以在HBase Shell中使用alter命令来修改其中的一些属性。
请注意，即便是在对区域进行过分割之后，也仍然需要在应用层对行键进行设计，以避免将过多连续的行键写入同一个区域。你需要仔细地选择，找到一种适合你的数据访问模式的分割算法。
该方法的另一个适用场景是用来提高将HBase表导出备份的数据导入回HBase的速度。
你可以使用一个简单的脚本来备份各区域的开始键。有了FileSplit类，我们可以使用原来的这些键来恢复HBase表的区域边界，然后再以使用导出数据文件进行导入的方式来恢复数据。
与将数据导入到同一个区域相比，这种方法能够显著提高数据恢复的速度，因为它会提前在多台RegionServer上恢复好多个区域，并且让这些区域均衡分布在多台RegionServer上。
 
# 避免写密集集群中的更新阻塞

在写密集的HBase集群中，你可能会发现有写入速度不稳定的现象。大多数写操作都非常快，但有些写操作却非常慢。对于一个在线系统来说，无论平均速度如何，这种写入速度的不稳定都是不可接受的。
导致这种情况的原因可能有两个：
* 分割/合并使得集群的负荷非常高
* 更新操作被RegionServer给阻塞住了

正如我们在“HBase基本性能调整”中描述的那样，你可以通过“禁用自动分割/合并，然后在低负载时手动进行分割/合并”的方式来避免分割/合并问题。
要在RegionServer日志中进行一下查找，如果发现很多带“Blocking updates for ...”字样的消息，说明很可能就有很多更新都被阻塞住了，这样的更新操作的响应时间可能就是非常差。
若要解决这类问题，必须同时对服务器端和客户端的配置作出调整，才能获得稳定的写入速度。

## 操作步骤

1.修改如下的属性值

vi $HBASE_HOME/conf/hbase-site.xml    
```
<property>    
<name>hbase.hregion.memstore.block.multiplier</name>    
<value>8</value>    
</property>
<property>
<name>hbase.hstore.blockingStoreFiles</name>
<value>20</value>
</property>
```

2.将修改同步到集群，然后重启hbase，使修改生效

## 工作原理


HBase写入操作的工作流程，如下图：

![](/images/bigdata/hbase/hbase_structure_2.jpg)


数据的修改首先会被写入到了RegionServer的HLog（WAL预写日志）中，以此方式来持久化在HDFS中。然后，该修改被传给了宿主HRegion，然后传入到其所属列族的HStore的内存空间里（即MemStore）。当MemStore的大小达到一定阀值时，数据修改会被写入到HDFS上的StoreFile中。StoreFiles在内部使用HFile文件格式来存储数据。
HBase是一种多版本并发控制（MVCC，Multiversion Concurrency Control）的架构。在更新/删除任何旧数据时，HBase都不会进行覆盖，而是给数据增加一个新版本。这使得HBase的写速度非常快，因为所有写操作都是顺序化的操作。

如果HDFS上的小StoreFile太多，HBase就会启动合并操作来将它们重写为几个大一些的StoreFile，同时减少StoreFile文件的数量。当一个区域的大小达到一定阀值时，它又会被分成两个部分。
为了防止合并/分割的时间过长并导致内存耗尽的错误，在某一区域的MemStore的大小达到一定阀值时，HBase会对更新进行阻塞。该阀值的定义为：
hbase.hregion.memstore.flush.size 乘以 hbase.hregion.memstore.block.multiplier

hbase.hregion.memstore.flush.size属性指定了MemStore在大小达到何值时会被写入到磁盘中。其默认值是128MB（0.90版本是64MB）。
hbase.hregion.memstore.block.multiplier属性的默认值是4，这意味着，如果MemStore的大小达到了512MB（128 * 4），该区域上的更新将被阻塞。对于写密集集群的更新高峰期来说，这个值太小了，所以我们要调高该阻塞阀值。

我们通常会让hbase.hregion.memstore.flush.size属性保持为默认值，然后将hbase.hregion.memstore.block.multiplier属性调整为一个较大的值（比如8），以此来提高MemStore阻塞阀值，减少更新阻塞发生的次数。
请注意，调高hbase.hregion.memstore.block.multiplier会增大写盘时出现合并/分割问题的可能性，所以调整时一定要小心。
第2步适用于另一种阻塞情况。如果任何一个Store的StoreFiles数（每个写盘MemStore一个StoreFile）超过了hbase.hstore.blockingStoreFiles（默认为10），那么该区域的更新就会被阻塞，直到合并完成或超过hbase.hstore.blockingWaitTime（默认90秒）所指定的时间为止。我们将它加大到了20，这对于写密集集群来说是一个相当大的值了。这样做的副作用是：会有更多的文件需要合并。

这种调整通常会减少更新阻塞的发生机会。然而，正如我们刚才提到的那样，它也有副作用。建议在调整这些设置时务必小心，调整期间务必时刻观察写操作的吞吐量和延迟时间，这样才能找到最佳的配置值。
 

# 调节MemStore内存大小

HBase的写操作会首先在所属区域的MemStore中生效，然后要等到MemStore的大小达到一定阀值之后才会被写入HDFS，从而省出一部分内存空间。MemStore的写盘操作以后台线程的方式运行，使用的是MemStore的快照。因此，即便是在MemStore写盘时，HBase仍可以一直处理写操作。这使得HBase的写入速度非常快。如果写操作的高峰值高到MemStore写盘都跟不上趟的程度，那么写操作填满MemStore的速度就会不断提高，而MemStore的总内存使用量也将继续升高。如果某一RegionServer中所有MemStore的总大小达到了某一可配置的阀值，更新又会被阻塞，并且强制进行写盘。

调整MemStore总体内存的大小以避免发生更新阻塞。

```
vi $HBASE_HOME/conf/hbase-site.xml    
<property>    
<name>hbase.regionserver.global.memstore.upperLimit</name>    
<value>0.45</value>    
</property>
<property>
<name>hbase.regionserver.global.memstore.lowerLimit</name>
<value>0.4</value>
</property>
```

hbase.regionserver.global.memstore.upperLimit属性控制了一台RegionServer中所有MemStore的总大小的最大值，超过该值后，新的更新就会被阻塞，并且强制进行写盘。这是一项可防止HBase在写入高峰时耗尽内存的配置信息。该属性默认为0.4，这意味着RegionServer堆大小的40%。

该默认值在很多情况下都可以很好地发挥作用。但是，如果RegionServer日志中出现了许多含有Flush of region xxxx due to global heap pressure的日志，那么可能就需要调整该属性来处理这种高写入速度了。

在第2步中，我们调整了hbase.regionserver.global.memstore.lowerLimit属性，它指定了何时对MemStores进行强制写盘，系统会一直进行写盘直到MemStore所占用的总内存大小低于该属性的值为止。其默认值是RegionServer堆大小的35%。
在写密集集群中，调高这两个值有助于减少更新因MemStore大小限制而被阻塞的机会。另一方面，调整时必须十分小心，避免引发全垃圾回收或出现内存耗尽的错误。
通常情况下，写密集集群上的读性能不像写性能那么重要。因此，我们可以为了优化写操作而调整集群。优化方法之一就是：减少分配给HBase块缓存的内存空间，将空间留给MemStore。关于如何调整块缓存的大小，请参见“调高读密集集群的块缓存大小”一节。
 

# 低延迟系统的客户端调节


前面两节介绍的是避免服务器端阻塞的方法。可以帮助集群稳定、高性能地运行。服务器端的调整可以显著改善集群的吞吐量和平均延迟时间。
然而，在低延迟系统和实时系统中，只进行服务器端的调优还不够。在低延迟系统中，长时间的停顿即使只是偶尔发生一两次也不可以接受。
为了避免长时间的停顿，还要对客户端进行一些调节。
在写密集集群中进行客户端调整的步骤如下。

```
vi $HBASE_HOME/conf/hbase-site.xml
<property>
  <name>hbase.client.pause</name>
  <value>10</value>
</property>
<property>
  <name>hbase.client.retries.number</name>
  <value>11</value>
</property> 
<property>
  <name>hbase.ipc.client.tcpnodelay</name>
  <value>true</value>
</property> 
<property>
  <name>ipc.ping.interval</name>
  <value>4000</value>
</property>
```

在第1步和第2步中调整hbase.client.pause和hbase.client.retries.number属性的目的是：让客户端在连接集群失败时能在很短的时间内迅速重试。

hbase.client.pause属性控制的是让客户端在两次重试之间休眠多久。其默认值是100毫秒（0.1秒）。hbase.client.retries.number属性用来指定最大重试次数。其默认值是35。
每两次重试之间的休眠时间可按下面这个公式计算得出。
pause_time = hbase.client.pause * RETRY_BACKOFF[retries]
其中，RETRY_BACKOFF是一个重试系数表，其定义如下。
public static int RETRY_BACKOFF[] = { 1, 1, 1, 2, 2, 4, 4, 8, 16, 32};
在重试10次以后，HBase就会一直使用最后一个系数（32）来计算休眠时间。
如果将暂停时间设为20毫秒，最大重试次数设为11，每两次连接集群重试之间的暂停时间将依次为：
{ 20, 20, 20, 40, 40, 80, 80, 160, 320, 640, 640}
这就意味着客户端将在2060毫秒内重试11次，然后放弃连接到集群。
在第3步中，我们将hbase.ipc.client.tcpnodelay设为了true。此设置将禁止使用Nagle算法来进行客户端和服务器之间的套接字传输。
Nagle算法是一种提高网络效率的手段，它会将若干较小的传出消息存在缓冲区中，然后再将它们一次全都发送出去。Nagle算法默认是启用的。低延迟系统应该将hbase.ipc.client.tcpnodelay设置为true，从而禁用Nagle算法。
在第4步中，我们将ipc.ping.interval设为了4000毫秒（4秒），这样客户端和服务器之间的套接字传输就不会超时。ipc.ping.interval的默认值是1分钟，对于低延迟系统来说，这有点太长了。
你也可以在客户端代码中使用org.apache.hadoop.hbase.HBaseConfiguration类来覆盖之前在hbase-site.xml中设置的各属性的值。下面代码可以实现与前面的第1步和第2步设定同样的效果。

```
Configuration conf = HBaseConfiguration.create();
conf.setInt("hbase.client.pause", 20);
conf.setInt("hbase.client.retries.number", 11);
HTable table = new HTable(conf, "tableName");

```


# 配置列族的块缓存

Hbase支持使用块缓存来提高读性能，在进行扫描时，如果块缓存功能已启用，并且仍有剩余空间，从hdfs上的storefile中读取到的数据块就会被缓存在RegionServer的Java堆空间中，以便以下一次访问同一块中的数据可以直接访问缓存中的块，块缓存有助于减少数据检索时的磁盘I/O

块缓存可以在表的列族一级进行配置，不同的列族可以有不同的缓存优先级，甚至还可以禁用块缓存，应用程序应根据数据规模和访问模式的不同而适当的利用这一缓存机制

下面介绍配置列族块缓存的方法和利用hbase块缓存的技巧


```
hbase(main):005:0> create 'test_c',{NAME=>'F1'},{NAME=>'F2',IN_MEMORY=>'true'},{NAME=>'F3',BLOCKCACHE=>'false'}
0 row(s) in 2.4910 seconds

=> Hbase::Table - test_c
hbase(main):006:0> describe 'test_c'

COLUMN FAMILIES DESCRIPTION                                                                                                               
{NAME => 'F1',  BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}       
{NAME => 'F2',  BLOCKSIZE => '65536', IN_MEMORY => 'true', BLOCKCACHE => 'true'}        
{NAME => 'F3',  BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'}      

#以上结果省略了一些字段，只是显示主要的字段

```

分别观察IN_MEMORY和BLOCKCACHE的情况


## 运行原理

从列族F1可以看到创建列族时，列族的块缓存是启用的（BLOCKCACHE => 'true'），而常驻内存是关闭的（IN_MEMORY => 'false'）

hbase块缓存的块优先级分为单次存取，多次存取和在内存中存取三种，如果有必要可以给块添加一个"内存中"标志，这样hbase更为积极的尽量将其保留在内存中（根据LRU,是最近最少使用），否则该块就处于单次存取优先级，如果该块又被再次存取，他就会被标记为多次存取级，分配这三个优先级的缓存空间并不相同，单次存取和内存中存取各占总缓存空间的25%，多次存取占50%

![](/images/bigdata/hbase/lru_cache.jpg)


在我们创建列族f2时，标明要将in_memory属性打开，因此，属于该列族的块都将以内存中优先级进行缓存，列族f3禁用了块缓存，这就意味着该列族的数据块不会被放入缓存中，我们不建议禁用块缓存


# 调高读密集集群的块缓存大小

在前面已经介绍过：要提高写性能，RegionServer需将大量的Java堆空间分配给MemStore；要提高读性能，RegionServer则需要将大量的堆空间来缓存storefile的文件块


## 操作步骤

1.修改块缓存大小

vim $hbase_home/conf/hbase-site.xml

```
<property>
	<name>hfile.block.cache.size</name>
	<value>0.4</value>
</property>
```


2.同步修改到整个集群，重启使之生效


## 运行原理

hfile.block.cache.size属性来指定将RegionServer堆空间最大值的多大百分比分配给块缓存，默认情况下，是分配堆空间啊 最大值的40%


在读密集型集群中，建议减少一些MemStore的空间，将更多的内存分配给块缓存，MemStore和块缓存加在一起通常会消耗RegionServer堆空间最大值的60%-70%，这是一个合理的值


# 客户端扫描类的设置

最重要的扫描类设置包括扫描缓存行数，扫描属性选择和扫描块缓存，下面介绍如何正确的配置


```
hbase.client.scanner.caching
Description
一次scan.next()获取的行数
这个参数会和“hbase.client.scanner.max.result.size”配合使用，默认值是Integer.MAX_VALUE

Default
2147483647


hbase.client.scanner.max.result.size
Description
调用一次scan.next()返回的最大字节数，默认是2M，如果一行超过了2M（默认值），仍然将这一行返回，如果网络带宽大的话，可以提高这个值

Default
2097152=2M

```

## 操作步骤
1.vim hbase-site.xml
```
<property>
	<name>hbase.client.scanner.caching</name>
	<value>2147483647</value>
</property>
```

2.只需要读取你需要的列，需指定列族和限定符（列），代码如下：

```
Scan scan = new Scan();
scan.addColumn(family1, qualifier1);
scan.addColumn(family1, qualifier2);
```

3.要对某个特定的扫描禁用块缓存，需将如下 内容添加到代码中（注意：这里并不是要禁用服务器的块缓存，而是要防止将扫描类扫描过的块放入缓存中，因为这些缓存是无效的，这些缓存只是针对该次扫描，不会有普遍性，所以放入缓存中只是浪费内存）

```
Scan scan = new Scan();
scan.setCacheBlocks(false);

```

## 运行原理

* 如果将返回的行数增大，则用于缓存这些行的内存也就越高，这样做也存在风险：就是在处理完数据集并调用扫描类的next（）方法之前客户端可能已经超时了，如果客户端处理过程很快，就可以将缓存数提高
* 尽量返回我们需要的列，不要返回无用的列族，所以不要使用scan.addFamily()将列族中的所有列都返回给客户端，而应该调用scan.addColumn()来指定哪些列需要返回
* 如果扫描类中禁用了缓冲功能，那么其扫描过的块就不会被添加到块缓冲中，禁用某个特定的扫描的块缓存有时非常重要，例如：当使用MapReduce对某张表进行全扫描的时候,会将缓存空间填满，并且将原来缓存中的内容使用LRU挤出,而这次扫描缓存的内容并不是常用的，所以缓存命中的时候，命中率不是很高了）


# 调整块大小来提高寻道性能

hbase数据存储在hfile格式的storefile中，storefile由hfile块组成，hfile块是hbase从storefile中读取数据时的最小数据单位，同时他也是RegionServer在块缓存中缓存数据的基本元素

为了获得更好的性能，我们应当根据平均键/值对规模和磁盘I/O速度的 不同来选择不同的块大小，像块缓存和Bloom过滤器一样，hfile块的大小也是可在列族级进行配置的

可以查看Hfile的使用参数

```
[root@hdp-node-01 ~]# /bigdata_installed/hbase/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile
usage: HFile [-a] [-b] [-e] [-f <arg> | -r <arg>] [-h] [-k] [-m] [-p]
       [-s] [-v] [-w <arg>]
 -a,--checkfamily         Enable family check
 -b,--printblocks         Print block index meta data
 -e,--printkey            Print keys
 -f,--file <arg>          File to scan. Pass full-path; e.g. #后面接hfile的路径
                          hdfs://a:9000/hbase/hbase:meta/12/34
 -h,--printblockheaders   Print block headers for each block.
 -k,--checkrow            Enable row order check; looks for out-of-order
                          keys
 -m,--printmeta           Print meta data of file #打印元数据
 -p,--printkv             Print key/value pairs #打印hfile中存储的额键值对
 -r,--region <arg>        Region to scan. Pass region name; e.g.
                          'hbase:meta,,1'
 -s,--stats               Print statistics
 -v,--verbose             Verbose output; emits file and meta data
                          delimiters
 -w,--seekToRow <arg>     Seek to this row and print all the kvs for this
                          row only

```


## 操作步骤

1.使用如下命令来显示hfile中的平均键/值对规模
```
[root@hdp-node-01 ~]# /bigdata_installed/hbase/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -m -f /hbase/data/default/test_table/86850c081862dd7a6e60f0b9da60d386/n/ee217584783e4a4aa3c3859c0319cb94   
2017-07-03 16:52:14,454 INFO  [main] hfile.CacheConfig: CacheConfig:disabled
Block index size as per heapsize: 1896
reader=/hbase/data/default/test_table/86850c081862dd7a6e60f0b9da60d386/n/ee217584783e4a4aa3c3859c0319cb94,
    compression=none,
    cacheConf=CacheConfig:disabled,
    firstKey=row2_#{i}/n:cc22/1498448254727/Put,
    lastKey=row2_132933/n:cc22/1498449727187/Put,
    avgKeyLen=27,
    avgValueLen=5,
    entries=33304,
    length=1580123
//...
Fileinfo:
    BLOOM_FILTER_TYPE = ROW
    DELETE_FAMILY_COUNT = \x00\x00\x00\x00\x00\x00\x00\x00
    EARLIEST_PUT_TS = \x00\x00\x01\x5C\xE2y\xD7\x07
    KEY_VALUE_VERSION = \x00\x00\x00\x01
    LAST_BLOOM_KEY = row2_132933
    MAJOR_COMPACTION_KEY = \xFF
    MAX_MEMSTORE_TS_KEY = \x00\x00\x00\x00\x00\x01S6
    MAX_SEQ_ID_KEY = 141843
    TIMERANGE = 1498448254727....1498449727187
    hfile.AVG_KEY_LEN = 27	#观察这两个参数的值，单位：字节
    hfile.AVG_VALUE_LEN = 5
    hfile.CREATE_TIME_TS = \x00\x00\x01\x5C\xFE\xE6\xABt
    hfile.LASTKEY = \x00\x0Brow2_132933\x01ncc22\x00\x00\x01\x5C\xE2\x90N\xD3\x04

```

从其中的输出可以看出hfile的平均键值对规模非常小（27字节），对于这种平均键值对规模非常小（100字节）的情况看，应该使用小数据块（例如16KB），以避免每个数据块存储的键值对过多，**块内键值对过多会增大块内寻址的延迟时间，因为块内寻址操作每次都要从块中的第一个键值对开始顺序进行查找**



2.创建表，指定列族的blocksize大小

```
hbase>create 'hly_temp', {NAME=>'n',BLOCKSIZE=>'16384'}
hbase>describe 'hly_temp'

```
上面将列族n的块大小指定为16384（16KB),默认是64KB，这里我们选择较小的块大小的目的是随机存取更快，而付出的代价是块索引变大，会消耗更多的内存，另一方面，如果平均键值对规模更大，或者此破案速度慢造成了性能瓶颈，那么就应该选择一个较大的块大小，以便使一次磁盘I/O能够读取出更多的数据来


# 启用Bloom过滤器来提高整体吞吐量


## 场景

如果没有bloom过滤器，确定某storefile是否包含某一行键的唯一办法就是检查该storefile的块索引，因为那里存储了该storefile的每个块的开始行健，待查找到额行健通常落在某两个块的开始键之间，如果是这样，hbase就必须读取该数据块并从该块的开始键开始扫描，这样才能确定该行健是否真的存在

这里的问题是：在主合并将其写入一个文件之前，会有相当多的storefile，因此，还有多个storefile都包含所查询行健的某些单元格（就是列族，即相同行健的不同列族存放在了不同的storefile中）

下图是storefile中存储数据，块索引和bloom过滤器的结构图：

![](/images/bigdata/hbase/bloom_filter.jpg)


由上图知：
* 有4个storefile
* 某些键的行（row-D行）仅存在于一个storefile中
* 有些键的行（row-F行）分散在很多个storefile中
* 虽然只有storefile1实际上包含了row-D,但hbase还是要读取storefile2和storefile3的第一个数据块，查找该块是否包含row-D的某些单元格（列），因为row-D也落在了第一个数据块的行健范围之内，如果有一种机制告诉hbase不用读取storefile2和3就好了
* bloom过滤器可以减少上面不必要的I/O


## 操作步骤

```
#建表的时候定义bloom过滤器
hbase(main):007:0> create 'test_bloom',{NAME=>'F1'},{NAME=>'F2','BLOOMFILTER'=>'ROW'},{NAME=>'F3',BLOOMFILTER=>'ROWCOL'}


#查看表结构
hbase(main):008:0> describe 'test_bloom'
#打印结果进过了简化                                                              
{NAME => 'F1',  BLOOMFILTER => 'ROW', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}        
{NAME => 'F2',  BLOOMFILTER => 'ROW', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}          
{NAME => 'F3',  BLOOMFILTER => 'ROWCOL', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}  
```

从上面的打印结果可以查看，默认是开启bloom过滤器，级别为“ROW”

使用bloom过滤器的目的是：使hbase能高效的找出一个storefile是否包含指定的行或单元格（列），而无需真正的加载该文件并扫描其数据块，bloom过滤器可能出现假阳性，这意味着一个查询虽然返回的是"a row is contained in a file"，但实际上并没有包含，但bloom过滤器不允许假阴性，如果一个查询返回“a row is not in a file”，那么该行就绝对不会在该文件中（其实出现这样的情况是：bloom过滤器使用的是**位图索引**中的0或者1来表示一个行健是否在一个storefile中，1表示存在，0表示不存在）

通常情况下，错误率是0.01（由is.storefile.bloom.error.rate)设置来配置，因此Bloom过滤器报告某storefile包含某行但实际并不包含的可能性为1%，若要减少错误率，需要使用更多的空间来存储Bloom过滤器

默认情况下，Bloom过滤器是启用的是行级过滤，他还有（行+列级）的Bloom过滤器，行级Bloom过滤器可以用来检查某行键是否包含在一个storefile中，而（行+列级）的Bloom过滤器可以告诉hbase某个单元格是否包含在一个storefile中，（行+列级）Bloom过滤器需要更多的磁盘空间，因为单元格的数量远远大于行的数量


使用Bloom过滤器的另一个优点是：他可以改善块缓存比，因为读取的数据是进过过滤的，那么缓存中的数据将是理想中的数据（即匹配度较高的数据），这样缓存中就存放的是我们真正查询的数据

不足之处：Bloom过滤器中的每个条目都要占用1字节的存储空间，如果单元格较小（例如：包括键值对信息开销在20个字节内），那么Bloom过滤器就将在文件中占用1/20，另一方面，如果平均单元格大小为1KB，Bloom过滤器也就只占文件大小的约1/1000，设storefile为1GB，那么过滤器只需要1MB的存储空间，因此，建议在小型单元格列族中禁用Bloom过滤器，而在大中型单元格列族中一直启用Bloom过滤器































