下图是hbase集群的结构图

![](/images/bigdata/hbase/hbase_structure.jpg)

* zookeeper集群为整个hbase集群提供了协调服务，负责处理主节点选择，根RegionServer查找和结点注册等事务
* HMaster并不承担什么繁重的任务，他的任务是进行region分配及故障转移，日志分割和负载均衡
* regionServer服务器负责存储具体的region，处理对于所托管的region的I/O请求，将内存中的数据存储（MemStore）写到hdfs中国，对region进行分隔与合并
* hdfs是hbase存储数据文件了（storefile）和预写日志（WAL)的地方，我们通常会让hbase的RegionServer与hdfs的DataNode运行在同一台机器上，但这也不是必须的



# 设置hadoop的分散磁盘I/O

现在的服务器通常都有多个磁盘设备，因此能够提供大容量的存储，这些磁盘通常都被设置为了RAID阵列，这种配置适用于许多场景，但hadoop除外

hadoop从节点把hdfs数据块和MapReduce临时文件都存储在了本地磁盘上，**使用多个独立磁盘可以分散磁盘I/O**，从而使本地磁盘操作从中受益


下面介绍如何设置hadoop使用多个磁盘来分散磁盘I/O

## 操作步骤

我们假设每个DataNode节点都有多个硬盘，这些磁盘按照JBOD（just a Bunch Of Disks,简单磁盘捆绑或磁盘簇）或RAID0的方式进行配置，假设这些磁盘分别被装载在了/mnt/d0,/mnt/d1.../mnt/dn目录上，而且hdfs的启动用户对于每个装载点都有写权限


1.在每个DataNode结点的每个磁盘上都为hdfs创建一个用于存储数据块的目录

```
hadoop$ mkdir -p /mnt/d0/dfs/data
hadoop$ mkdir -p /mnt/d1/dfs/data
....
hadoop$ mkdir -p /mnt/dn/dfs/data

```


2.在hdfs配置文件（hdfs-site.xml）中添加如下这段内容

```
<property>
	<name>dfs.data.dri</name>
	<value>/mnt/d0/dfs/data,/mnt/d1/dfs/data,.../mnt/dn/dfs/data</vlaue>
</property>

```

3.将修改后的hdfs-site.xml文件同步到整个集群中

```
hadoop@master$ for slave in `cat $hadoop_home/etc/hadoop/slaves`;do
rsync -avz  $hadoop_home/etc/hadoop/slaves  $slave:$hadoop_home/etc/hadoop/slaves
done

```

4.重启hdfs

```
$hadoop_home/bin/stop-dfs.sh
$hadoop_home/bin/start-dfs.sh

```

## 运行原理

我们建议将DataNode磁盘做成JBOD或者RAID0，因为我们并不需要RAID的冗余能力，hdfs可以通过节点间的复制来确保数据冗余，因此即使单个磁盘发生故障，也不会造成数据丢失

无论是配置成JBOD还是RAID0，这些磁盘都要装载在不同的路径上，这里的重点是dfs.data.dir属性要包含各磁盘上创建的所有目录，DataNode保存本地数据块的位置由dfs.data.dirs属性指定，通过将该属性设为逗号分隔的多个目录，DataNode就可以以循环的方式将自己的数据块存储到所有的磁盘中，这样hadoop就可以有效的将磁盘I/O分散到所有磁盘上了


## 补充说明

如果你还运行了MapReduce，而MapReduce又把自己的临时文件存储在了TaskTracker的本地文件系统上，那么你可能还要对MapReduce进行一番配置，让他也把磁盘I/O分散开来

1.在每个TaskTracker（NodeManager）节点的每个磁盘上都为MapReduce创建一个用于存储中间数据文件的目录

```
hadoop$ mkdir -p /mnt/d0/mapred/local
hadoop$ mkdir -p /mnt/d1/mapred/local
....                            
hadoop$ mkdir -p /mnt/dn/mapred/local

```

2.在MapReduce的配置文件了（mapred-site.xml)中添加以下内容

```
<property>
	<name>mapred.local.dir</name>
	<vlaue>/mnt/d0/mapred/local,/mnt/d1/mapred/local...,/mnt/dn/mapred/local</value>
</property>

```


3.将修改后的mapred-site.xml文件同步到整个集群中，然后重启yarn


在执行过程中，MapReduce会在各TaskTracker节点的本地磁盘上产生大量临时文件，与hdfs一样，将目录设在不同的磁盘上有助于MapReduce将磁盘I/O分散开


# 使用网络拓扑结构脚本使hadoop可感知机架

hadoop有一个“机架感知”（Rack Awareness）的概念，管理员可以定义集群中每个DataNode节点所处的机架，hadoop的机架感知能力非常重要，因为：
* 机架感知可以防止数据丢失
* 机架感知可提高网络的性能

## 操作步骤

1.在hadoop配置目录下创建一个topology.sh脚本，根据你的实际情况修改第3行中的topology.data文件的路径

vim $hadoop_home/etc/hadoop/topology.sh

```
while [ $# -gt 0 ] ;do
	nodeArg=$1
	exec< /usr/local/hadoop/current/conf/topology.data
	result=""
	while read line ; do
		ar=( $line )
		if [ "${ar[0]}" = "$nodeArg" ] ;then
			result="${ar[1]}"
		fi
	done
	shift 
	if [ -z "$result" ] ;then
		echo -n '/default/rack'
	else
		echo -n "$result"
	fi
done
```

给脚本加上执行权限

```
chmod +x $hadoop_home/etc/hadoop/topology.sh

```

2.创建一个包含如下内容的topology.data文件，根据你的实际情况修改其中的IP地址和机架

vim $hadoop_home/etc/hadoop/topology.data

```
10.161.30.108	/dc1/rack1
10.161.221.198	/dc1/rack2
10.161.19.149	/dc1/rack3

```

3.在hadoop配置文件（core-site.xml）中添加下面几行

vim $hadoop_home/etc/hadoop/core-site.xml

```
<property>
	<name>topology.script.file.name</name>
	<value>$hadoop_home/etc/hadoop/topology.sh</value>
</property>
```


4.将修改后的文件同步到整个集群中，然后重启hdfs和MapReduce

5.确认hdfs现在是否已可感知机架，如果一切正常，应该能够在NameNode日志文件中找到类似于下面这样的内容


![](/images/bigdata/hbase/rack_topology.jpg)


6.确认MapReduce现在是否可感知机架，如果一切正常，应该能够在JobTaskTracker（NodeManager）日志文件中找到类似下面的内容

![](/images/bigdata/hbase/rack_topology_2.jpg)


## 运行原理

下图就是hadoop的机架感知概念图

![](/images/bigdata/hbase/rack_topology_3.jpg)


hdfs文件的每个数据块都会复制到多个DataNode节点上，这样做的目的是防止在一台机器发生故障时丢失所有数据拷贝，然而，如果数据的所有副本都碰巧只被复制到了同一个机架内的几个DataNode节点上，那么该机架出故障时，该数据的所有副本也会丢失，因此，为了避免这种情况，NameNode需要知道网络拓扑结构，以便用此信息来进行智能化的数据复制

上图所示的默认复制系数为3，其中两份数据放在同一机架的不同机器上，另外一份数据放在其他机架的一台机器上，这样就能确保一个机架的故障不会导致数据所有副本的丢失

在正常情况下，与不同机架内的两台机器相比，同一机架内的两台机器之间的带宽更宽，延迟也更低，有了网络拓扑信息，hadoop就能选择从适当的DataNode中读取数据，从而最大限度的提高网络性能，如果数据在本地机器上，hadoop就会从本地读取数据，如果本地没有该数据，hadoop就会尝试从同一机架内的机器上读取数据，如果本机架内没有他才会从其他机架的机器上读取数据

在第1步中，我们创建了一个topology.sh脚本，该脚本以DNS名为参数，可以在输出中返回网络拓扑（机架）的名称，DNS名与网络拓扑名之间的映射由第2步所创建的topology.data文件来提供，如果在topology.data文件中找不到某个DNS名，该脚本则会返回默认机架名/default/rack

在第3步中，我们在core-site.xml中设置了topology.script.file.name属性，这样hadoop就会调用topology.sh来将DNS名称解析成网络拓扑名




# 以noatime和nodiratime方式装载磁盘

如果所装载的磁盘只供hadoop使用，而且你使用的又是ext3,ext4或XFS文件系统，那么就应该以带noatime和nodiratime属性的方式装载磁盘


如果装载磁盘时带有noatime参数，那么在读取文件系统上的文件时就不会更新其**访问时间戳**，如果使用了nodiratime属性，所装载的磁盘就不更新目录inode在文件系统中的访问时间，因为没有更新访问时间戳所需的磁盘I/O，所以文件系统的读取也加快了


## 操作步骤

假设hadoop只用到了两个磁盘：/dev/xvdc和/dev/xvdd，这两个磁盘分别被装载在/mnt/is1和/mnt/is2目录上，另外假设你所使用的是ext3文件系统

在集群的每个节点执行一遍以下操作，以noatime和nodiratime方式装载磁盘

1.在/etc/fstab 文件中添加下列设置
sudo vim /etc/fatab

```
/dev/xvdc /mnt/is1 ext3 defaults,noatime,nodiratime 0 0 
/dev/xvdd /mnt/is2 ext3 defaults,noatime,nodiratime 0 0 
```

2.卸载磁盘，然后再重新装载磁盘以使改变生效

```
sudo umount /dev/xvdc
sudo umount /dev/xvdd

sudo mount /dev/xvdc
sudo mount /dev/xvdd

```

3.检查装载选项是否生效

```
mount 
/dev/xvdc on /mnt/is1 type ext3 (rw,noatime,nodiratime)
/dev/xvdd on /mnt/is2 type ext3 (rw,noatime,nodiratime)

```


## 运行原理

因为hdfs使用了NameNode来管理其文件系统的元数据（iNode），所以任何hadoop保存的访问时间信息都与各文件块的atime属性无关，因此，DataNode本地文件系统中的访问时间戳就没有什么意义，这就是我们建议以noatime和nodiratime方式装载磁盘的原因，如果磁盘纯粹仅供hadoop使用的话，以noatime和nodiratime方式装载磁盘可在每次本地文件访问时节省一些磁盘写I/O

## 补充说明

另一种优化方法是减少ext3或ext4文件系统的保留块百分比，在默认情况下，文件系统会保留一些文件块来供某些特权进程使用，这种保留的目的是为了防止出现“用户进程已将磁盘空间填满，而系统守护进程为了继续工作还需要申请一些磁盘空间”的情况，对于那些用于安装操作系统的磁盘来说，这一点非常重要，但对于那些仅供hadoop使用的磁盘来说，这就用处不大了

在通常情况下，这些仅供hadoop使用的磁盘都有着非常大的存储空间，减少保留块百分比可以为hdfs集群增肌不少存储容量，在通常情况下，默认的保留块百分比都是为5%，我们可以将其降低为1%

**不要降低用于安装操作系统的磁盘上的保留块比例**


如果要降低保留块百分比，需要在每个节点的每个磁盘上执行以下命令：

```
[root@hdp-node-01 ~]# tune2fs -m 1 /dev/sda1
tune2fs 1.41.12 (17-May-2010)
Setting reserved blocks percentage to 1% (5120 blocks)
[root@hdp-node-01 ~]# 

```


# 将vm.swappiness设为0以避免交换

如果内存页在一定时间内没有被访问过，即使仍有足够的可用内存，linux也会将其移到交换（Swap）空间中，这就叫做换出（swap out),反过来，将换出的数据从交换空间读入内存的操作叫做换入（swap in），在很多情况下，交换是必要的，但在交换时Java虚拟机（JVM)可能会行为异常，如果Hbase被交换出去的话，Hbase可能就会出现问题，zookeeper会话过期就是交换可能引起的一个典型问题

## 操作步骤

在集群的每个节点上都把如下步骤执行一遍，以调整Linux参数，避免发生交换

1.将vm.swappiness参数设置为0

```
sysctl -w vm.swappiness=0
```
以上方法惠子下一次重启服务器的时候失效

2.将如下内容添加到/etc/sysctl.conf文件中，以便每次系统启动都会启用该设置
```
echo "vm.swappiness=0">>/etc/sysctl.conf
```

## 运行原理

vm.swappiness参数可用来将内存页交换到磁盘中的主动程度，他可以接受0到100之间的任何值（值较低表明内核进行交换的频率低），而值较高则会令内核更频繁的将应用程序交换出去，其默认值是60


将vm.swappiness设为0，这会让内核在尽可能长的时间内避免将进程交换出物理内存，这对hbase是件好事，因为hbase进程会消耗大量的内存，vm.swappiness的值越高，hbase被换出的机会就越多，还会引发非常慢的垃圾回收过程，这就可能导致RegionServer进程因zookeeper会话超时而被强行终止，我们建议把该参数设为0或其他较小的值（10），然后观察交换的状态



# Java GC和Hbase堆的设置

本节将介绍hbase JVM堆的一些重要参数设置，并介绍如何启用和理解GC日志，还将介绍一些为Hbase而调整Java GC设置的通用准则

## 操作步骤

1.给Hbase分配足够大的堆，例如下面的代码就给Hasee配置了一个800MB的堆内存大小

vim $HBASE_HOME/conf/hbase-env.sh

```
export HBASE_HEAPSIZE=8000

```

2.启用GC日志功能

vim $HBASE_HOME/conf/hbase-env.sh
```
export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/usr/local/hbase/log/gc-hbase.log -XX:+UseGCLogFileRotation -XX:NumberOf
GCLogFiles=1 -XX:GCLogFileSize=512M"
```

需要将/usr/local/hbase/log目录创建出来
```
mkdir /usr/local/hbase/log -p

```

3.添加如下代码来比缺省设定更早的启动CMS（Concurrent-Mark-Sweep，并发-标记-清除）GC算法

vim $HBASE_HOME/conf/hbase-env.sh

```
export HBASE_OPTS="$HBASE_OPTS -XX:CMSInitiatingOccupancyFraction=60"
```

4.将修改同步到整个集群中，然后重启hbase

5.查看输出日志中的GC日志

cat /usr/local/hbase/log/gc-hbase.log 

## 运行原理

在第1步中，我们配置了Hbase的堆内存大小，在默认情况下，hbase会使用的堆大小为1GB，这对于当今的电脑硬件来说太低了，hbase的堆大小应该超过4GB，我们建议是8GB或者更大，但不能超过16GB

在第2步中，我们启用了JVM的日志功能，要理解日志输出，就必须了解JVM的内存分配和垃圾回收的基本知识，如下图就是JVM的分代式垃圾回收系统

![](/images/bigdata/hbase/jvm.jpg)


堆分为三代：Perm（持久代），Old（老年代或终身代）和Young（年轻代），年轻代由三个独立的空间组成，Eden空间和两个survivor（存活）空间：S0和S1

通常情况下，对象会被分配在年轻代的Eden空间中，如果分配失败（Eden已满），则会停止所有的Java线程，然后调用年轻代GC（Minor GC，次回收），年轻代（Eden和S0空间）中的所有存活对象都被复制到S1空间中，如果S1空间也满了，则把对象复制（升级）到老年代，如果升级失败，则调用一次老年代回收（Major/Full GC,主回收，全回收），持久代和老年代通常在一起进行回收，持久代用于保存对象的类定义和方法定义


在查看的GC日志中，通常有如下的输出格式

![](/images/bigdata/hbase/gc_formatted.jpg)

```
2017-07-01T21:30:19.975+0800: 28.608: 
[GC2017-07-01T21:30:19.975+0800: 28.608: [ParNew: 8959K->956K(8960K), 0.1029590 secs] 19894K->12056K(28864K), 0.1031220 secs]
[Times: user=0.19 sys=0.01, real=0.10 secs] 
```

以下是字段的说明

* <timestamp>是GC发生的时间，这是一个相对于应用程序启动后的时间
* <collector>是次回收所使用回收算法的内部名称
* <starting occupancy1>是回收前年轻代占用空间的大小
* <ending occupancy1>是回收后年轻代占用空间的大小
* <pause time1>是次回收的暂停时间（单位为秒）
* <starting occupancy3>是回收前整个堆占用空间的大小
* <ending occupancy3>是回收后整个堆占用空间的大小
* <pause time3>是整个垃圾回收过程中的暂停时间，这将包括一个主回收的时间
* [Time:]解释GC的时间都花在那些方面：用户时间，系统时间，实际花费的时间


接下来我们来看看CMS GC日志，hbase将CMS GC作为老年代的默认垃圾回收算法

CMS GC会按如下步骤执行

1.初始标记
2.并发标记
3.重新标记
4.并发清除

CMS只会在初始标记和重新标记阶段才会停止应用程序的线程，在并发标记和清除阶段，CMS线程会与应用程序的线程一起运行


后面的看不懂。。。。。


# 使用压缩

数据压缩的原因：
* 数据压缩可减少读写HDFS的字节数
* 数据压缩可节省磁盘空间
* 在从远程服务器获取数据时，数据压缩可提高网络带宽的效率


## 操作步骤

1.安装压缩库（需要上网查找）

没有完善

2.在hbase-site.xml文件中添加hbase.regionserver.codes配置

vim $HBASE_HOME/conf/hbase-site.xml

```
<property>
	<name>hbase.regionserver.codecs</name>
	<value>lzo,gz,snappy</value>
</property>
```


3.测试

hbase带有一个工具，可测试压缩程序是否设置正确，使用此工具在集群的每个节点上测试LZO的设置，如果一切配置正确，其输出显示SUCCESS

```
$HBASE_HOME/bin/hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://host/path/to/hbase snappy
```


4.测试表是否使用了压缩

```
hbase>create 't1',{NAME='cf1',COMPRESSION=>'LZO'}

hbase>describe 't1'
#可以看其中的COMPRSSION='LZO', 就说明使用了压缩

```

## 说明

为了避免有节点启动时没有安装编解码器或者安装不当，我们在hbase-site.xml文件中hbase.regionserver.codecs 设置了LZO等压缩类型，次设置会导致服务器在lzo安装不正确的情况下无法启动，如果看到类似“could not load native gpl library(无法加载原生的GPL库）”这样的日志，就说明lzo安装有问题，要解决问题，请确认原生lzo库是否已安装以及安装路径是否配置正确



# 管理合并

这里介绍的是major compaction

hbase 表有如下的物理存储结构

![](/images/bigdata/hbase/major_compaction.jpg)


hbase表由多个region组成，当写数据的时候，会先写WAL,然后是写MemStore，当MemStore的大小达到一定的阈值时，数据修改会被写入到hdfs中的storefile上

随着数据的增加，hdfs上可能会有很多storefile，这对hdfs的性能很不利，因此hbase会自动选取一些较小的storefile进行合并，这个过程叫做（minor compaction),在某些情况下，或者在每隔配置时间出触发一次（默认每7天一次）的情况下，会自动运行主合并（major compaction），主合并会删除那些已经标记为删除或者过期的单元格，并且将存储中的所有的storefile重写入一个storefile中

然而，因为主合并要重写所有的store中的数据，所以在此过程中会出现大量的磁盘I/O和网络通信，对于重负载的系统来说，这是不可接受的，你可能更愿意让他在系统负荷较低的时候运行

本节将介绍如何关闭主合并和手动执行主合并

## 操作步骤

1.禁用主合并

vim $HBASE_HOME/conf/hbase-site.xml
```
<property>
<name>hbase.hregion.majorcompaction</name>
<value>0</value>
</property
```

2.将修改同步到整个集群中，然后重新启动hbase

3.现在主合并被禁用了，我们需要手动执行主合并

major_compact 的使用

```
		  Compact all regions in a table:
          hbase> major_compact 't1'
          hbase> major_compact 'ns1:t1'
          Compact an entire region:
          hbase> major_compact 'r1'
          Compact a single column family within a region:
          hbase> major_compact 'r1', 'c1'
          Compact a single column family within a table:
          hbase> major_compact 't1', 'c1'
		 
```

```
echo "major_compact 'regionName'" |$HBASE_HOME/bin/hbase shell
```

现在我们要合并test_table表的一个region

查看test_table表的所有region

![](/images/bigdata/hbase/major_compaction_2.jpg)

查看每个region对应的storefile的数量

```
#可以看到test_table有两个region
[root@hdp-node-01 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb

#查看每个region对应的storefile
[root@hdp-node-01 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/a2*/n
-rw-r--r--   3 root supergroup     225797 2017-06-27 11:55 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6/n/2d6247b7ddc24987a9841a4796d60e41
-rw-r--r--   3 root supergroup     479697 2017-06-27 11:55 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6/n/aa213298844d4b13a9bbe1c1390b5217

[root@hdp-node-01 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/ff*/n
-rw-r--r--   3 root supergroup    3017119 2017-06-27 11:55 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb/n/120fd5d047074eb8b35ab94fa9507b9b
-rw-r--r--   3 root supergroup     209968 2017-06-27 11:55 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb/n/dc5a02e951b24a3baf4de94250b966fc

```

执行合并操作

```
#这里指定的是hdfs中region的名字
hbase(main):006:0> major_compact 'a23e6a902233c23c1c7846c953b301e6'

```

再次查看region的情况

```
#2个region
[root@hdp-node-03 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-07-01 23:37 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb

#查看每个region的情况
[root@hdp-node-03 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6/n
-rw-r--r--   4 root supergroup     688744 2017-07-01 23:37 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6/n/c9192f8aabfd4acc957011abd4d0376e
#可以看出这个region已经进行了合并

[root@hdp-node-03 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb/n
-rw-r--r--   3 root supergroup    3017119 2017-06-27 11:55 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb/n/120fd5d047074eb8b35ab94fa9507b9b
-rw-r--r--   3 root supergroup     209968 2017-06-27 11:55 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb/n/dc5a02e951b24a3baf4de94250b966fc

```

当然也可以对整张表进行major compaction

```
hbase(main):007:0> major_compact 'test_table'
```


## 补充说明

* 在低负载时进行手动执行合并，你可以在一个crontab中配置major_compact命令定时执行
* 另外你可以使用hbase的API执行合并



# 管理region分隔

一张hbase表在开始时通常只有一个region（如果没有进行预处理），然而随着数据的不断增加，当该region达到配置所设定的最大值时，就会自动分隔成两部分，以便能处理更多的数据，如下图所以

![](/images/bigdata/hbase/region_split.jpg)



以上是hbase默认的分隔方式，在有些情况下这种机制会引发分隔/合并风暴的问题


因为数据的分布和增长大致都是均匀的，所以表中的所哟region最终会在同一时间需要分隔，紧接着在分隔之后，又会对这些子region进行合并，将他们的数据重写到不同的文件中，这回导致大量的磁盘I/O和网络通信


为了避免这种情况，你可以关闭自动分隔，然后手动执行分隔，因为你可以控制在何时进行分隔，所以这样做有助于将I/O负载分散开来，另外，手动分隔让你能够更好的控制region，他也可以帮助你追踪和修复一些与region相关的问题

本节将介绍如何关闭自动region分隔和手动调用region分隔

## 操作步骤


1.将region的最大限制调大（100G）

vim hbase-site.xml

```
<property>
<name>hbase.hregion.max.filesize</name>
<value>107374182400</value>
</property>
```

2.将修改同步到整个集群中

3.手动触发region 分隔

经过上述设置之后，region分隔不会在region大小达到已经配置100GB阈值之前发生

```
hbase(main):008:0> split

ERROR: wrong number of arguments (0 for 1)

Here is some help for this command:
Split entire table or pass a region to split individual region.  With the 
second parameter, you can specify an explicit split key for the region.  
Examples:
    split 'tableName'
    split 'namespace:tableName'
    split 'regionName' # format: 'tableName,startKey,id' 这里hbase的帮助文档有误，不是这样的格式，其实就是hdfs中region的名字
    split 'tableName', 'splitKey'
    split 'regionName', 'splitKey'
```

手动分隔前

```
[root@hdp-node-03 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-07-01 23:37 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb

```

执行手动分隔

```
echo "split 'a23e6a902233c23c1c7846c953b301e6'"|$hbase_home/bin/hbase shell
```

手动分隔后

```
[root@hdp-node-03 ~]# /bigdata_installed/hadoop/bin/hdfs dfs -ls /hbase/data/default/test_table
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tabledesc
drwxr-xr-x   - root supergroup          0 2017-06-27 11:55 /hbase/data/default/test_table/.tmp
drwxr-xr-x   - root supergroup          0 2017-07-02 00:05 /hbase/data/default/test_table/86850c081862dd7a6e60f0b9da60d386
drwxr-xr-x   - root supergroup          0 2017-07-02 00:05 /hbase/data/default/test_table/905cbe31e1774ab5a6997be8e3dd0d99
drwxr-xr-x   - root supergroup          0 2017-07-01 23:37 /hbase/data/default/test_table/a23e6a902233c23c1c7846c953b301e6
drwxr-xr-x   - root supergroup          0 2017-07-02 00:05 /hbase/data/default/test_table/ff6d315691778b4b155c0cd61751caeb
[root@hdp-node-03 ~]# 
```

为什么会有4个region，因为有一个region是不可用的，就是我们制定分隔的那个region

我们看web界面就可以看出

![](/images/bigdata/hbase/region_split_2.jpg)













