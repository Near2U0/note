[toc]



> 内核：linux-5.7

# 自旋锁数据结构

```c
 71 typedef struct spinlock {
 72     union {
 73         struct raw_spinlock rlock;
 74 
 75 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 76 # define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
 77         struct {
 78             u8 __padding[LOCK_PADSIZE];
 79             struct lockdep_map dep_map;
 80         };
 81 #endif
 82     };
 83 } spinlock_t;
```

```c
 20 typedef struct raw_spinlock {
 21     arch_spinlock_t raw_lock;
 22 #ifdef CONFIG_DEBUG_SPINLOCK
 23     unsigned int magic, owner_cpu;
 24     void *owner;
 25 #endif
 26 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 27     struct lockdep_map dep_map;
 28 #endif
 29 } raw_spinlock_t;


// arch_spinlock_t 的定义如下
 15 #ifdef CONFIG_DEBUG_SPINLOCK
 16 
 17 typedef struct {
 18     volatile unsigned int slock;
 19 } arch_spinlock_t; 
 23 #else
 25 typedef struct { } arch_spinlock_t;
 29 #endif
     
```

# 初始化

```c
333 # define spin_lock_init(lock)                   \
334 do {                                \
335     static struct lock_class_key __key;         \
336                                 \
337     __raw_spin_lock_init(spinlock_check(lock),      \
338                  #lock, &__key, LD_WAIT_CONFIG);    \
339 } while (0)

 // 初始化如下
 16 void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,
 17               struct lock_class_key *key, short inner)
 18 {
 19 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 20     /* 
 21      * Make sure we are not reinitializing a held lock:
 22      */
 23     debug_check_no_locks_freed((void *)lock, sizeof(*lock));
 24     lockdep_init_map_wait(&lock->dep_map, name, key, 0, inner);
 25 #endif
 26     lock->raw_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED; // 见如下
 27     lock->magic = SPINLOCK_MAGIC;
 28     lock->owner = SPINLOCK_OWNER_INIT;
 29     lock->owner_cpu = -1;
 30 }
    
    
 15 #ifdef CONFIG_DEBUG_SPINLOCK
 16 
 17 typedef struct {
 18     volatile unsigned int slock;
 19 } arch_spinlock_t;
 20 
 21 #define __ARCH_SPIN_LOCK_UNLOCKED { 1 }  // SMP中初始化为1
 22 
 23 #else
 24 
 25 typedef struct { } arch_spinlock_t;
 26 
 27 #define __ARCH_SPIN_LOCK_UNLOCKED { }
 28 
 29 #endif
```

# 申请和释放自旋锁

## 申请

```c
376 static __always_inline void spin_lock_irq(spinlock_t *lock)
377 {
378     raw_spin_lock_irq(&lock->rlock);
379 }

// 最后调用到了这里
124 static inline void __raw_spin_lock_irq(raw_spinlock_t *lock)
125 {
126     local_irq_disable(); // 关闭中断 ， 最终是 cli 关闭中断
127     preempt_disable();  // 关闭抢占，让改进程不会调度出去
128     spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
129     LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
130 }


565 #define LOCK_CONTENDED(_lock, try, lock)            \
566 do {                                \
567     if (!try(_lock)) {   // do_raw_spin_trylock(_lock)                \
568         lock_contended(&(_lock)->dep_map, _RET_IP_);    \
569         lock(_lock);    // do_raw_spin_lock(_lock)                \
570     }                           \
571     lock_acquired(&(_lock)->dep_map, _RET_IP_);         \
572 } while (0)

    
 35 static inline int arch_spin_trylock(arch_spinlock_t *lock)
 36 {
 37     char oldval = lock->slock;
 38 
 39     lock->slock = 0;
 40     barrier();
 41 
 42     return oldval > 0;
 43 }

179 static inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)
180 {
181     __acquire(lock);
182     arch_spin_lock(&lock->raw_lock);
183     mmiowb_spin_lock();
184 }

 29 static inline void arch_spin_lock(arch_spinlock_t *lock)
 30 {
 31     lock->slock = 0;
 32     barrier();
 33 }

```





## 释放

// 略





>  单CPU加锁和解锁只是将中断关闭和打开即可，但是如果是多CPU，那么还需要关闭抢占，通过CPU在空转，等待锁的释放

# 关闭和打开中断之前，保存状态

```c
spin_lock_irqsave(lock, flag)
spin_unlock_irqsave(lock, flag)
    
// 会将中断的状态保存在flag中
```



